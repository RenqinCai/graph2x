{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sw/centos/anaconda3/2019.10/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punct = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer(\"This is a sentence\")\n",
    "print(type(tokens[0].text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 lines loaded.\n",
      "200000 lines loaded.\n",
      "300000 lines loaded.\n",
      "Finish loading train dataset, totally 302573 lines.\n",
      "10000 lines loaded.\n",
      "20000 lines loaded.\n",
      "30000 lines loaded.\n",
      "40000 lines loaded.\n",
      "Finish loading test dataset, totally 40730 lines.\n"
     ]
    }
   ],
   "source": [
    "dir_path = '../Dataset/ratebeer/large_500'\n",
    "# Load train dataset\n",
    "train_review = []\n",
    "cnt = 0\n",
    "file_path = os.path.join(dir_path, 'train_review_filtered.json')\n",
    "with open(file_path) as f:\n",
    "    for line in f:\n",
    "        line_data = json.loads(line)\n",
    "        user_id = line_data['user']\n",
    "        item_id = line_data['item']\n",
    "        rating = line_data['rating']\n",
    "        review = line_data['review']\n",
    "        train_review.append([item_id, user_id, rating, review])\n",
    "        cnt += 1\n",
    "        if cnt % 100000 == 0:\n",
    "            print('{} lines loaded.'.format(cnt))\n",
    "print('Finish loading train dataset, totally {} lines.'.format(len(train_review)))\n",
    "# Load test dataset\n",
    "test_review = []\n",
    "cnt = 0\n",
    "file_path = os.path.join(dir_path, 'test_review_filtered.json')\n",
    "with open(file_path) as f:\n",
    "    for line in f:\n",
    "        line_data = json.loads(line)\n",
    "        user_id = line_data['user']\n",
    "        item_id = line_data['item']\n",
    "        rating = line_data['rating']\n",
    "        review = line_data['review']\n",
    "        test_review.append([item_id, user_id, rating, review])\n",
    "        cnt += 1\n",
    "        if cnt % 10000 == 0:\n",
    "            print('{} lines loaded.'.format(cnt))\n",
    "print('Finish loading test dataset, totally {} lines.'.format(len(test_review)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert List Data to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data = pd.DataFrame(train_review, columns=['item', 'user', 'rating', 'review'])\n",
    "df_test_data = pd.DataFrame(test_review, columns=['item', 'user', 'rating', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item</th>\n      <th>user</th>\n      <th>rating</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>196</td>\n      <td>999</td>\n      <td>17</td>\n      <td>dark brown body with a light brown head . nutt...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1149</td>\n      <td>999</td>\n      <td>14</td>\n      <td>hazy orange / gold body is topped by a medium ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>236</td>\n      <td>999</td>\n      <td>16</td>\n      <td>12 oz bottle thanks to acknud . pours much dar...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>634</td>\n      <td>999</td>\n      <td>15</td>\n      <td>clear and radiant mahogany body with a small b...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>490</td>\n      <td>999</td>\n      <td>16</td>\n      <td>nice looker with tons of spiderweb lacing . bo...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>302568</th>\n      <td>373</td>\n      <td>3849</td>\n      <td>9</td>\n      <td>this shit is rejected coors light , i m certai...</td>\n    </tr>\n    <tr>\n      <th>302569</th>\n      <td>1458</td>\n      <td>3849</td>\n      <td>10</td>\n      <td>being a philly fan , i m supposed to hate anyt...</td>\n    </tr>\n    <tr>\n      <th>302570</th>\n      <td>269</td>\n      <td>3849</td>\n      <td>10</td>\n      <td>a good malt liquor , undeniably , but it was l...</td>\n    </tr>\n    <tr>\n      <th>302571</th>\n      <td>859</td>\n      <td>3849</td>\n      <td>2</td>\n      <td>how this beer is actually better than regular ...</td>\n    </tr>\n    <tr>\n      <th>302572</th>\n      <td>779</td>\n      <td>3849</td>\n      <td>3</td>\n      <td>slightly better than the beast , but damn , ho...</td>\n    </tr>\n  </tbody>\n</table>\n<p>302573 rows Ã— 4 columns</p>\n</div>",
      "text/plain": "        item  user  rating                                             review\n0        196   999      17  dark brown body with a light brown head . nutt...\n1       1149   999      14  hazy orange / gold body is topped by a medium ...\n2        236   999      16  12 oz bottle thanks to acknud . pours much dar...\n3        634   999      15  clear and radiant mahogany body with a small b...\n4        490   999      16  nice looker with tons of spiderweb lacing . bo...\n...      ...   ...     ...                                                ...\n302568   373  3849       9  this shit is rejected coors light , i m certai...\n302569  1458  3849      10  being a philly fan , i m supposed to hate anyt...\n302570   269  3849      10  a good malt liquor , undeniably , but it was l...\n302571   859  3849       2  how this beer is actually better than regular ...\n302572   779  3849       3  slightly better than the beast , but damn , ho...\n\n[302573 rows x 4 columns]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2963"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_data['user'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3744"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_data['item'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute Sentence TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catDoc(textlist):\n",
    "    res = []\n",
    "    for tlist in textlist:\n",
    "        res.extend(tlist)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tfidf_embedding(text, feature_word_list):\n",
    "    \"\"\"\n",
    "    :param: text: list, sent_number * word\n",
    "    :return: \n",
    "        vectorizer: \n",
    "            vocabulary_: word2id\n",
    "            get_feature_names(): id2word\n",
    "        tfidf: array [sent_number, max_word_number]\n",
    "    \"\"\"\n",
    "    vectorizer = CountVectorizer(lowercase=True, vocabulary=feature_word_list)\n",
    "    word_count = vectorizer.fit_transform(text)\n",
    "    tfidf_transformer = TfidfTransformer()\n",
    "    tfidf = tfidf_transformer.fit_transform(word_count)\n",
    "    tfidf_weight = tfidf.toarray()\n",
    "    return vectorizer, tfidf_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_score(text, feature_word_list):\n",
    "    vectorizer = CountVectorizer(lowercase=True, vocabulary=feature_word_list)\n",
    "    word_count = vectorizer.fit_transform(text)\n",
    "    return word_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_array(a, id2word, vocab):\n",
    "    \"\"\"\n",
    "    :param a: matrix, [N, M], N is document number, M is word number\n",
    "    :param id2word: word id to word\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    # Loop over documents\n",
    "    for i in range(len(a)):\n",
    "        d[i] = {}\n",
    "        # Loop over words\n",
    "        for j in range(len(a[i])):\n",
    "            if a[i][j] != 0:\n",
    "                wid_voc = vocab[id2word[j]]\n",
    "                d[i][wid_voc] = a[i][j]\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Feature Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_2_id_file = '../Dataset/ratebeer/large_500/train/feature/feature2id.json'\n",
    "with open(feature_2_id_file, 'r') as f:\n",
    "    feature_vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2000"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'1'"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vocab['aroma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature words: 2000\n"
     ]
    }
   ],
   "source": [
    "feature_word_list = list(feature_vocab.keys())\n",
    "print('Number of feature words: {}'.format(len(feature_word_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Whether there are reviews with no sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_data = 0\n",
    "for idx, row in df_train_data.iterrows():\n",
    "    review_text = row['review']\n",
    "    review_sents = sent_tokenize(review_text)\n",
    "    if len(review_sents) == 0:\n",
    "        print(row)\n",
    "        invalid_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(invalid_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Sentence Vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 10000 lines\n",
      "Processed 20000 lines\n",
      "Processed 30000 lines\n",
      "Processed 40000 lines\n",
      "Processed 50000 lines\n",
      "Processed 60000 lines\n",
      "Processed 70000 lines\n",
      "Processed 80000 lines\n",
      "Processed 90000 lines\n",
      "Processed 100000 lines\n",
      "Processed 110000 lines\n",
      "Processed 120000 lines\n",
      "Processed 130000 lines\n",
      "Processed 140000 lines\n",
      "Processed 150000 lines\n",
      "Processed 160000 lines\n",
      "Processed 170000 lines\n",
      "Processed 180000 lines\n",
      "Processed 190000 lines\n",
      "Processed 200000 lines\n",
      "Processed 210000 lines\n",
      "Processed 220000 lines\n",
      "Processed 230000 lines\n",
      "Processed 240000 lines\n",
      "Processed 250000 lines\n",
      "Processed 260000 lines\n",
      "Processed 270000 lines\n",
      "Processed 280000 lines\n",
      "Processed 290000 lines\n",
      "Processed 300000 lines\n",
      "Finish.\n"
     ]
    }
   ],
   "source": [
    "# sentence vocab\n",
    "sentence_count = dict()\n",
    "sentence_with_no_feature = 0\n",
    "# Loop for each review\n",
    "for idx, row in df_train_data.iterrows():\n",
    "    review_text = row['review']\n",
    "    review_sents = sent_tokenize(review_text)\n",
    "    tf_score = get_tf_score(review_sents, feature_word_list)\n",
    "    # _, tf_score = get_tfidf_embedding(review_sents, feature_word_list)\n",
    "    tfidf_sum_sents = np.sum(tf_score, axis=1)\n",
    "    for i in range(len(review_sents)):\n",
    "        if tfidf_sum_sents[i] != 0.0:\n",
    "            cur_sent = review_sents[i]\n",
    "            # check whether this sentence has more than 3 tokens\n",
    "            tokens = tokenizer(cur_sent)\n",
    "            cnt_tokens = 0\n",
    "            for token in tokens:\n",
    "                if token.text.isdigit() or (token.text in punct):\n",
    "                    pass\n",
    "                else:\n",
    "                    cnt_tokens += 1\n",
    "            # only sentence with more than 2 effective tokens can be added into the sentence vocab\n",
    "            if cnt_tokens < 3:\n",
    "                pass\n",
    "            else:\n",
    "                sentence_count[cur_sent] = 1 + sentence_count.get(cur_sent, 0)\n",
    "        else:\n",
    "            sentence_with_no_feature += 1\n",
    "    if (idx+1) % 10000 == 0:\n",
    "        print(\"Processed {} lines\".format(idx+1))\n",
    "print('Finish.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1246458"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3696"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_with_no_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort sentence based on counts (the majority should be 1)\n",
    "sorted_sent_counts = sorted(sentence_count.items(), key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_vocab_list = list(sentence_count.keys())\n",
    "# Building mappings from sentences to ids and ids to sentences\n",
    "sent_to_id = {entry[0]: str(id) for (id, entry) in enumerate(sorted_sent_counts)}\n",
    "# Since we loaded all the tokenized sentences, we don't need to add the special UNK token\n",
    "id_to_sent = {str(id): sent for (sent, id) in sent_to_id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1246458"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1246458"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id_to_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'nice bitter finish .'"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_sent['42']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Sentence to ID into Json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/large_500/train/sentence/id2sentence.json', 'w') as f:\n",
    "    json.dump(id_to_sent, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/large_500/train/sentence/sentence2id.json', 'w') as f:\n",
    "    json.dump(sent_to_id, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Sentence Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item</th>\n      <th>user</th>\n      <th>rating</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>196</td>\n      <td>999</td>\n      <td>17</td>\n      <td>dark brown body with a light brown head . nutt...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1149</td>\n      <td>999</td>\n      <td>14</td>\n      <td>hazy orange / gold body is topped by a medium ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>236</td>\n      <td>999</td>\n      <td>16</td>\n      <td>12 oz bottle thanks to acknud . pours much dar...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>634</td>\n      <td>999</td>\n      <td>15</td>\n      <td>clear and radiant mahogany body with a small b...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>490</td>\n      <td>999</td>\n      <td>16</td>\n      <td>nice looker with tons of spiderweb lacing . bo...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>302568</th>\n      <td>373</td>\n      <td>3849</td>\n      <td>9</td>\n      <td>this shit is rejected coors light , i m certai...</td>\n    </tr>\n    <tr>\n      <th>302569</th>\n      <td>1458</td>\n      <td>3849</td>\n      <td>10</td>\n      <td>being a philly fan , i m supposed to hate anyt...</td>\n    </tr>\n    <tr>\n      <th>302570</th>\n      <td>269</td>\n      <td>3849</td>\n      <td>10</td>\n      <td>a good malt liquor , undeniably , but it was l...</td>\n    </tr>\n    <tr>\n      <th>302571</th>\n      <td>859</td>\n      <td>3849</td>\n      <td>2</td>\n      <td>how this beer is actually better than regular ...</td>\n    </tr>\n    <tr>\n      <th>302572</th>\n      <td>779</td>\n      <td>3849</td>\n      <td>3</td>\n      <td>slightly better than the beast , but damn , ho...</td>\n    </tr>\n  </tbody>\n</table>\n<p>302573 rows Ã— 4 columns</p>\n</div>",
      "text/plain": "        item  user  rating                                             review\n0        196   999      17  dark brown body with a light brown head . nutt...\n1       1149   999      14  hazy orange / gold body is topped by a medium ...\n2        236   999      16  12 oz bottle thanks to acknud . pours much dar...\n3        634   999      15  clear and radiant mahogany body with a small b...\n4        490   999      16  nice looker with tons of spiderweb lacing . bo...\n...      ...   ...     ...                                                ...\n302568   373  3849       9  this shit is rejected coors light , i m certai...\n302569  1458  3849      10  being a philly fan , i m supposed to hate anyt...\n302570   269  3849      10  a good malt liquor , undeniably , but it was l...\n302571   859  3849       2  how this beer is actually better than regular ...\n302572   779  3849       3  slightly better than the beast , but damn , ho...\n\n[302573 rows x 4 columns]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_vocab_is_same(sklearn_vocab, feature_vocab):\n",
    "    if len(sklearn_vocab) == len(feature_vocab):\n",
    "        for key, value in sklearn_vocab.items():\n",
    "            sklearn_vocab_id = value\n",
    "            feature_vocab_id = feature_vocab[key]\n",
    "            if int(feature_vocab_id) == sklearn_vocab_id:\n",
    "                continue\n",
    "            else:\n",
    "                return False\n",
    "    else:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'80'"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_to_id['sampled from bottle .']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1246458"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sent_to_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_text_list = list(sent_to_id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['bottle at home .',\n 'easy to drink .',\n 'small white head .',\n 'very good beer .',\n 'very nice beer .',\n 'alcohol is well hidden .',\n 'on tap at the brewery .',\n 'lots of carbonation .',\n 'poured from bottle .',\n 'taste is the same as aroma .']"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_text_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntvector, tfidf_weight = get_tfidf_embedding(sentence_text_list, feature_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(1246458, 2000)"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_vocab_is_same(cntvector.vocabulary_, feature_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 50000 lines\n",
      "Processed 100000 lines\n",
      "Processed 150000 lines\n",
      "Processed 200000 lines\n",
      "Processed 250000 lines\n",
      "Processed 300000 lines\n",
      "Processed 350000 lines\n",
      "Processed 400000 lines\n",
      "Processed 450000 lines\n",
      "Processed 500000 lines\n",
      "Processed 550000 lines\n",
      "Processed 600000 lines\n",
      "Processed 650000 lines\n",
      "Processed 700000 lines\n",
      "Processed 750000 lines\n",
      "Processed 800000 lines\n",
      "Processed 850000 lines\n",
      "Processed 900000 lines\n",
      "Processed 950000 lines\n",
      "Processed 1000000 lines\n",
      "Processed 1050000 lines\n",
      "Processed 1100000 lines\n",
      "Processed 1150000 lines\n",
      "Processed 1200000 lines\n",
      "Finish. Totally 1246458 lines\n",
      "Totally 1246458 sentences has at least 1 feature and 0 sentences don't have feature.\n"
     ]
    }
   ],
   "source": [
    "sentence_to_feature = dict()\n",
    "sentence_with_no_feature = 0\n",
    "tfidf_sum_sents = np.sum(tfidf_weight, axis=1)\n",
    "for i in range(len(sentence_text_list)):\n",
    "    cur_sent = sentence_text_list[i]\n",
    "    # if this sentence is in the sent_to_id vocabulary\n",
    "    assert cur_sent in sent_to_id\n",
    "    # get the sentence_id (str)\n",
    "    cur_sent_id = sent_to_id[cur_sent]\n",
    "    assert int(cur_sent_id) == i\n",
    "    # find all the feature that has non-zero tf-idf weight\n",
    "    feature_dict = dict()\n",
    "    for j in range(len(tfidf_weight[i])):\n",
    "        if tfidf_weight[i][j] != 0.0:\n",
    "            # get the feature\n",
    "            feature_id = str(j)\n",
    "            feature = feature_word_list[j]\n",
    "            feature_tfidf = tfidf_weight[i][j]\n",
    "            feature_dict[feature_id] = feature_tfidf\n",
    "    if len(feature_dict) > 0:\n",
    "        sentence_to_feature[cur_sent_id] = feature_dict\n",
    "    else:\n",
    "        sentence_with_no_feature += 1\n",
    "    if (i+1) % 50000 == 0:\n",
    "        print(\"Processed {} lines\".format(i+1))\n",
    "print(\"Finish. Totally {} lines\".format(i+1))\n",
    "print(\"Totally {} sentences has at least 1 feature and {} sentences don't have feature.\".format(\n",
    "    len(sentence_to_feature), sentence_with_no_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sentence to feature\n",
    "# sentence_to_feature = dict()\n",
    "# sentence_with_no_feature = 0\n",
    "# # Loop for each review\n",
    "# for idx, row in df_train_data.iterrows():\n",
    "#     review_text = row['review']\n",
    "#     review_sents = sent_tokenize(review_text)\n",
    "#     cntvector, tfidf_weight = get_tfidf_embedding(review_sents, feature_word_list)\n",
    "#     # assert cntvector.vocabulary_ == feature_vocab\n",
    "#     assert check_vocab_is_same(cntvector.vocabulary_, feature_vocab)\n",
    "#     # print(tfidf_weight)\n",
    "#     tfidf_sum_sents = np.sum(tfidf_weight, axis=1)\n",
    "#     for i in range(len(review_sents)):\n",
    "#         cur_sent = review_sents[i]\n",
    "#         # if this sentence is in the sent_to_id vocabulary\n",
    "#         if cur_sent in sent_to_id:\n",
    "#             # get the sentence_id\n",
    "#             cur_sent_id = sent_to_id[cur_sent]\n",
    "#             # find all the feature that has no-zero tf-idf weight\n",
    "#             feature_dict = dict()\n",
    "#             for j in range(len(tfidf_weight[i])):\n",
    "#                 if tfidf_weight[i][j] != 0.0:\n",
    "#                     # get the feature\n",
    "#                     feature_id = j\n",
    "#                     feature = feature_word_list[j]\n",
    "#                     feature_tfidf = tfidf_weight[i][j]\n",
    "#                     feature_dict[feature_id] = feature_tfidf\n",
    "#             if len(feature_dict) > 0:\n",
    "#                 sentence_to_feature[cur_sent_id] = feature_dict\n",
    "#             else:\n",
    "#                 sentence_with_no_feature += 1\n",
    "\n",
    "#     if (idx+1) % 10000 == 0:\n",
    "#         print(\"Processed {} lines\".format(idx+1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Sentence2Feature into Json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/large_500/train/sentence/sentence2feature.json', 'w') as f:\n",
    "    json.dump(sentence_to_feature, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/large_500/train/sentence/sentence2feature.json', 'r') as f:\n",
    "    sentence_to_feature = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'10': 0.4639189217945095, '486': 0.8858776631121363}"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_feature['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "{'195': 0.6551843674842948, '315': 0.7554690229282766}"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_to_feature['1246457']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1246458"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_to_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feature_per_sentence = []\n",
    "for key, value in sentence_to_feature.items():\n",
    "    num_feature_per_sentence.append(len(value))\n",
    "    assert len(value) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of features per sentence: 5.817189989554401\n",
      "Max number of features per sentence: 44\n",
      "Min number of features per sentence: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean number of features per sentence: {}\".format(np.mean(num_feature_per_sentence)))\n",
    "print(\"Max number of features per sentence: {}\".format(np.max(num_feature_per_sentence)))\n",
    "print(\"Min number of features per sentence: {}\".format(np.min(num_feature_per_sentence)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get User to Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item</th>\n      <th>user</th>\n      <th>rating</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>196</td>\n      <td>999</td>\n      <td>17</td>\n      <td>dark brown body with a light brown head . nutt...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1149</td>\n      <td>999</td>\n      <td>14</td>\n      <td>hazy orange / gold body is topped by a medium ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>236</td>\n      <td>999</td>\n      <td>16</td>\n      <td>12 oz bottle thanks to acknud . pours much dar...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>634</td>\n      <td>999</td>\n      <td>15</td>\n      <td>clear and radiant mahogany body with a small b...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>490</td>\n      <td>999</td>\n      <td>16</td>\n      <td>nice looker with tons of spiderweb lacing . bo...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>302568</th>\n      <td>373</td>\n      <td>3849</td>\n      <td>9</td>\n      <td>this shit is rejected coors light , i m certai...</td>\n    </tr>\n    <tr>\n      <th>302569</th>\n      <td>1458</td>\n      <td>3849</td>\n      <td>10</td>\n      <td>being a philly fan , i m supposed to hate anyt...</td>\n    </tr>\n    <tr>\n      <th>302570</th>\n      <td>269</td>\n      <td>3849</td>\n      <td>10</td>\n      <td>a good malt liquor , undeniably , but it was l...</td>\n    </tr>\n    <tr>\n      <th>302571</th>\n      <td>859</td>\n      <td>3849</td>\n      <td>2</td>\n      <td>how this beer is actually better than regular ...</td>\n    </tr>\n    <tr>\n      <th>302572</th>\n      <td>779</td>\n      <td>3849</td>\n      <td>3</td>\n      <td>slightly better than the beast , but damn , ho...</td>\n    </tr>\n  </tbody>\n</table>\n<p>302573 rows Ã— 4 columns</p>\n</div>",
      "text/plain": "        item  user  rating                                             review\n0        196   999      17  dark brown body with a light brown head . nutt...\n1       1149   999      14  hazy orange / gold body is topped by a medium ...\n2        236   999      16  12 oz bottle thanks to acknud . pours much dar...\n3        634   999      15  clear and radiant mahogany body with a small b...\n4        490   999      16  nice looker with tons of spiderweb lacing . bo...\n...      ...   ...     ...                                                ...\n302568   373  3849       9  this shit is rejected coors light , i m certai...\n302569  1458  3849      10  being a philly fan , i m supposed to hate anyt...\n302570   269  3849      10  a good malt liquor , undeniably , but it was l...\n302571   859  3849       2  how this beer is actually better than regular ...\n302572   779  3849       3  slightly better than the beast , but damn , ho...\n\n[302573 rows x 4 columns]"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_user = df_train_data.groupby('user')\n",
    "user_id_list = []\n",
    "user_reviews = []\n",
    "# Loop over all user\n",
    "for user_df_chunk in list(group_by_user):\n",
    "    user_id = int(user_df_chunk[0])\n",
    "    user_df = user_df_chunk[1]\n",
    "    user_text = \" \".join(list(user_df['review']))\n",
    "    user_id_list.append(user_id)\n",
    "    user_reviews.append(user_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2963"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2963"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2000"
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_word_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute User TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntvector_user, tfidf_weight_user = get_tfidf_embedding(user_reviews, feature_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assert cntvector_user.vocabulary_ == feature_vocab\n",
    "check_vocab_is_same(cntvector_user.vocabulary_, feature_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(2963, 2000)"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_weight_user.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 user processed.\n",
      "1000 user processed.\n",
      "1500 user processed.\n",
      "2000 user processed.\n",
      "2500 user processed.\n",
      "Totally 2963 users\n"
     ]
    }
   ],
   "source": [
    "user_to_feature = dict()\n",
    "for i in range(len(user_id_list)):\n",
    "    feature_dict = dict()\n",
    "    cur_user_id = user_id_list[i]\n",
    "    for j in range(len(tfidf_weight_user[i])):\n",
    "        if tfidf_weight_user[i][j] != 0.0:\n",
    "            # get the feature\n",
    "            # NOTE: make sure that the feature_id is str format\n",
    "            feature_id = str(j)\n",
    "            feature = feature_word_list[j]\n",
    "            feature_tfidf = tfidf_weight_user[i][j]\n",
    "            feature_dict[feature_id] = feature_tfidf\n",
    "    assert len(feature_dict) > 0\n",
    "    user_to_feature[str(cur_user_id)] = feature_dict\n",
    "    if (i+1) % 500 == 0:\n",
    "        print(\"{} user processed.\".format(i+1))\n",
    "print(\"Totally {} users\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2963"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_to_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feature_per_user = []\n",
    "for key,value in user_to_feature.items():\n",
    "    num_feature_per_user.append(len(value))\n",
    "    assert len(value) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of features per user: 436.17448531893353\n",
      "Max number of features per user: 1481\n",
      "Min number of features per user: 12\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean number of features per user: {}\".format(np.mean(num_feature_per_user)))\n",
    "print(\"Max number of features per user: {}\".format(np.max(num_feature_per_user)))\n",
    "print(\"Min number of features per user: {}\".format(np.min(num_feature_per_user)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save User to Feature Mapping into Json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/large_500/train/user/user2feature.json', 'w') as f:\n",
    "    json.dump(user_to_feature, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/large_500/train/user/user2feature.json', 'r') as f:\n",
    "    user_to_feature = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Item to Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy Item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_item = df_train_data.groupby('item')\n",
    "item_id_list = []\n",
    "item_reviews = []\n",
    "# Loop over all user\n",
    "for item_df_chunk in list(group_by_item):\n",
    "    item_id = str(item_df_chunk[0])\n",
    "    item_df = item_df_chunk[1]\n",
    "    item_text = \" \".join(list(item_df['review']))\n",
    "    item_id_list.append(item_id)\n",
    "    item_reviews.append(item_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3744"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(item_id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3744"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(item_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3744"
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_data['item'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Item TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "cntvector_item, tfidf_weight_item = get_tfidf_embedding(item_reviews, feature_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# assert cntvector_item.vocabulary_ == feature_vocab\n",
    "check_vocab_is_same(cntvector_item.vocabulary_, feature_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(3744, 2000)"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_weight_item.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 items processed.\n",
      "1000 items processed.\n",
      "1500 items processed.\n",
      "2000 items processed.\n",
      "2500 items processed.\n",
      "3000 items processed.\n",
      "3500 items processed.\n",
      "Finish. Totally 3744 items\n"
     ]
    }
   ],
   "source": [
    "item_to_feature = dict()\n",
    "for i in range(len(item_id_list)):\n",
    "    feature_dict = dict()\n",
    "    cur_item_id = item_id_list[i]\n",
    "    for j in range(len(tfidf_weight_item[i])):\n",
    "        if tfidf_weight_item[i][j] != 0.0:\n",
    "            # get the feature\n",
    "            feature_id = str(j)\n",
    "            feature = feature_word_list[j]\n",
    "            assert feature_id == feature_vocab[feature]\n",
    "            feature_tfidf = tfidf_weight_item[i][j]\n",
    "            feature_dict[feature_id] = feature_tfidf\n",
    "    assert len(feature_dict) > 0\n",
    "    item_to_feature[cur_item_id] = feature_dict\n",
    "    if (i+1) % 500 == 0:\n",
    "        print(\"{} items processed.\".format(i+1))\n",
    "print('Finish. Totally {} items'.format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3744"
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(item_to_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_feature_per_item = []\n",
    "for key,value in item_to_feature.items():\n",
    "    num_feature_per_item.append(len(value))\n",
    "    assert len(value) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of features per item: 461.50267094017096\n",
      "Max number of features per item: 1391\n",
      "Min number of features per item: 100\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean number of features per item: {}\".format(np.mean(num_feature_per_item)))\n",
    "print(\"Max number of features per item: {}\".format(np.max(num_feature_per_item)))\n",
    "print(\"Min number of features per item: {}\".format(np.min(num_feature_per_item)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Item to Feature Mapping into Json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/large_500/train/item/item2feature.json', 'w') as f:\n",
    "    json.dump(item_to_feature, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('2019.10': virtualenv)",
   "name": "python374jvsc74a57bd020bbdedb0079e23a85211f14a09dbcc829fefc965ff02508ebf68fe08b48d387"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "20bbdedb0079e23a85211f14a09dbcc829fefc965ff02508ebf68fe08b48d387"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}