{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sw/centos/anaconda3/2019.10/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import re\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 lines loaded.\n",
      "20000 lines loaded.\n",
      "30000 lines loaded.\n",
      "40000 lines loaded.\n",
      "50000 lines loaded.\n",
      "60000 lines loaded.\n",
      "70000 lines loaded.\n",
      "80000 lines loaded.\n",
      "90000 lines loaded.\n",
      "100000 lines loaded.\n",
      "110000 lines loaded.\n",
      "120000 lines loaded.\n",
      "130000 lines loaded.\n",
      "140000 lines loaded.\n",
      "150000 lines loaded.\n",
      "160000 lines loaded.\n",
      "170000 lines loaded.\n",
      "180000 lines loaded.\n",
      "190000 lines loaded.\n",
      "200000 lines loaded.\n",
      "210000 lines loaded.\n",
      "220000 lines loaded.\n",
      "230000 lines loaded.\n",
      "240000 lines loaded.\n",
      "250000 lines loaded.\n",
      "260000 lines loaded.\n",
      "270000 lines loaded.\n",
      "280000 lines loaded.\n",
      "290000 lines loaded.\n",
      "300000 lines loaded.\n",
      "Finish loading train dataset, totally 302573 lines.\n",
      "10000 lines loaded.\n",
      "20000 lines loaded.\n",
      "30000 lines loaded.\n",
      "40000 lines loaded.\n",
      "Finish loading test dataset, totally 40730 lines.\n"
     ]
    }
   ],
   "source": [
    "dir_path = '../Dataset/ratebeer/large_500'\n",
    "# Load train dataset\n",
    "train_review = []\n",
    "cnt = 0\n",
    "file_path = os.path.join(dir_path, 'train_review_filtered.json')\n",
    "with open(file_path) as f:\n",
    "    for line in f:\n",
    "        line_data = json.loads(line)\n",
    "        user_id = line_data['user']\n",
    "        item_id = line_data['item']\n",
    "        rating = line_data['rating']\n",
    "        review = line_data['review']\n",
    "        train_review.append([item_id, user_id, rating, review])\n",
    "        cnt += 1\n",
    "        if cnt % 10000 == 0:\n",
    "            print('{} lines loaded.'.format(cnt))\n",
    "print('Finish loading train dataset, totally {} lines.'.format(len(train_review)))\n",
    "# Load test dataset\n",
    "test_review = []\n",
    "cnt = 0\n",
    "file_path = os.path.join(dir_path, 'test_review_filtered.json')\n",
    "with open(file_path) as f:\n",
    "    for line in f:\n",
    "        line_data = json.loads(line)\n",
    "        user_id = line_data['user']\n",
    "        item_id = line_data['item']\n",
    "        rating = line_data['rating']\n",
    "        review = line_data['review']\n",
    "        test_review.append([item_id, user_id, rating, review])\n",
    "        cnt += 1\n",
    "        if cnt % 10000 == 0:\n",
    "            print('{} lines loaded.'.format(cnt))\n",
    "print('Finish loading test dataset, totally {} lines.'.format(len(test_review)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert List Data to Pandas Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data = pd.DataFrame(train_review, columns=['item', 'user', 'rating', 'review'])\n",
    "df_test_data = pd.DataFrame(test_review, columns=['item', 'user', 'rating', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item</th>\n      <th>user</th>\n      <th>rating</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>196</td>\n      <td>999</td>\n      <td>17</td>\n      <td>dark brown body with a light brown head . nutt...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1149</td>\n      <td>999</td>\n      <td>14</td>\n      <td>hazy orange / gold body is topped by a medium ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>236</td>\n      <td>999</td>\n      <td>16</td>\n      <td>12 oz bottle thanks to acknud . pours much dar...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>634</td>\n      <td>999</td>\n      <td>15</td>\n      <td>clear and radiant mahogany body with a small b...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>490</td>\n      <td>999</td>\n      <td>16</td>\n      <td>nice looker with tons of spiderweb lacing . bo...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>302568</th>\n      <td>373</td>\n      <td>3849</td>\n      <td>9</td>\n      <td>this shit is rejected coors light , i m certai...</td>\n    </tr>\n    <tr>\n      <th>302569</th>\n      <td>1458</td>\n      <td>3849</td>\n      <td>10</td>\n      <td>being a philly fan , i m supposed to hate anyt...</td>\n    </tr>\n    <tr>\n      <th>302570</th>\n      <td>269</td>\n      <td>3849</td>\n      <td>10</td>\n      <td>a good malt liquor , undeniably , but it was l...</td>\n    </tr>\n    <tr>\n      <th>302571</th>\n      <td>859</td>\n      <td>3849</td>\n      <td>2</td>\n      <td>how this beer is actually better than regular ...</td>\n    </tr>\n    <tr>\n      <th>302572</th>\n      <td>779</td>\n      <td>3849</td>\n      <td>3</td>\n      <td>slightly better than the beast , but damn , ho...</td>\n    </tr>\n  </tbody>\n</table>\n<p>302573 rows Ã— 4 columns</p>\n</div>",
      "text/plain": "        item  user  rating                                             review\n0        196   999      17  dark brown body with a light brown head . nutt...\n1       1149   999      14  hazy orange / gold body is topped by a medium ...\n2        236   999      16  12 oz bottle thanks to acknud . pours much dar...\n3        634   999      15  clear and radiant mahogany body with a small b...\n4        490   999      16  nice looker with tons of spiderweb lacing . bo...\n...      ...   ...     ...                                                ...\n302568   373  3849       9  this shit is rejected coors light , i m certai...\n302569  1458  3849      10  being a philly fan , i m supposed to hate anyt...\n302570   269  3849      10  a good malt liquor , undeniably , but it was l...\n302571   859  3849       2  how this beer is actually better than regular ...\n302572   779  3849       3  slightly better than the beast , but damn , ho...\n\n[302573 rows x 4 columns]"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of user in trainset: 2963\n",
      "number of item in trainset: 3744\n"
     ]
    }
   ],
   "source": [
    "print(\"number of user in trainset: {}\".format(len(list(df_train_data['user'].unique()))))\n",
    "print(\"number of item in trainset: {}\".format(len(list(df_train_data['item'].unique()))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def catDoc(textlist):\n",
    "    res = []\n",
    "    for tlist in textlist:\n",
    "        res.extend(tlist)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calTFidf(text):\n",
    "    # FIXED: Omit STOP Words\n",
    "    vectorizer = CountVectorizer(lowercase=True, stop_words='english')\n",
    "    wordcount = vectorizer.fit_transform(text)\n",
    "\n",
    "    print(\"wordcount: \", wordcount.shape)\n",
    "    tf_idf_transformer = TfidfTransformer()\n",
    "    tfidf_matrix = tf_idf_transformer.fit_transform(wordcount)\n",
    "    return vectorizer, tfidf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "for idx, row in df_train_data.iterrows():\n",
    "    text = row['review']\n",
    "    documents.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "302573"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute TF-IDF Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wordcount:  (302573, 87900)\n"
     ]
    }
   ],
   "source": [
    "vectorizer, tfidf_matrix = calTFidf(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of example is 302573, and the TFIDF vocabulary size is 87900\n"
     ]
    }
   ],
   "source": [
    "print(\"The number of example is {0}, and the TFIDF vocabulary size is {1}\".format(\n",
    "    len(documents), len(vectorizer.vocabulary_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Mean TF-IDF score for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tfidf = np.array(tfidf_matrix.mean(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(1, 87900)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([1.60342261e-04, 2.92155393e-05, 2.13455265e-06, 8.52083795e-07,\n       8.94606634e-07, 1.54518204e-06, 2.40607564e-06, 3.45788165e-05,\n       1.36548918e-06, 9.05476767e-07, 4.34640838e-05, 1.41078352e-06,\n       4.22942232e-05, 6.80602743e-07, 1.48317784e-05, 3.72549987e-07,\n       1.48041600e-06, 1.93395367e-06, 7.45418140e-07, 1.44921904e-06])"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tfidf[0][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort Words According to Their Mean TF-IDF value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_order = np.argsort(-word_tfidf[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(87900,)"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_order.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "array([37807,  6347,  9120, 31742, 76268, 46627, 48780, 53941, 77191,\n       39499, 11956, 85685, 31214, 61061, 21996, 35609, 13064, 14720,\n       18539, 10532])"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_order[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punct = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "22\n",
      "2008\n",
      "2011\n",
      "500\n",
      "2009\n",
      "10\n",
      "750\n",
      "2010\n",
      "2007\n",
      "11\n",
      "2006\n",
      "33\n",
      "2005\n",
      "50\n",
      "330\n",
      "2004\n",
      "08\n",
      "2003\n",
      "07\n",
      "25\n",
      "09\n",
      "16\n",
      "20\n",
      "06\n",
      "15\n",
      "30\n",
      "13\n",
      "18\n",
      "341\n",
      "14\n",
      "24\n",
      "05\n",
      "17\n",
      "21\n",
      "99\n",
      "27\n",
      "23\n",
      "355\n",
      "28\n",
      "75\n",
      "19\n",
      "29\n",
      "04\n",
      "03\n",
      "26\n",
      "650\n",
      "02\n",
      "31\n",
      "375\n",
      "01\n",
      "2002\n",
      "100\n",
      "40\n",
      "90\n"
     ]
    }
   ],
   "source": [
    "vocab2id = {}\n",
    "id2vocab = {}\n",
    "\n",
    "id2word = vectorizer.get_feature_names()\n",
    "feature_word_num = 2000\n",
    "feature_word_idx = 0\n",
    "\n",
    "cnt = 0\n",
    "for idx in word_order:\n",
    "    # get the word\n",
    "    word = id2word[idx]\n",
    "    # check if this word is a number/punctuation\n",
    "    if word.isdigit() or (word in punct):\n",
    "        print(word)\n",
    "    else:\n",
    "        # write word into vocab2id mapping\n",
    "        vocab2id[word] = str(len(vocab2id))\n",
    "        # write word into id2vocab mapping\n",
    "        id2vocab[str(cnt)] = word\n",
    "        cnt += 1\n",
    "        if cnt == feature_word_num:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the feature vocabulary to json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2000"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2000"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(id2vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/large_500/train/feature/feature2id.json', 'w') as f:\n",
    "    json.dump(vocab2id, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/large_500/train/feature/id2feature.json', 'w') as f:\n",
    "    json.dump(id2vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "key_cnt = 0\n",
    "stack_key = []\n",
    "for key, value in vocab2id.items():\n",
    "    if key_cnt == 10:\n",
    "        # write this into file\n",
    "        with open('../Dataset/ratebeer/large_500/train/feature/features.txt', 'a') as f:\n",
    "            key_text = '\\t'.join(stack_key)\n",
    "            f.write(key_text)\n",
    "            f.write('\\n')\n",
    "        key_cnt = 0\n",
    "        stack_key = []\n",
    "    stack_key.append(key)\n",
    "    key_cnt += 1\n",
    "\n",
    "if len(stack_key) != 0:\n",
    "    with open('../Dataset/ratebeer/large_500/train/feature/features.txt', 'a') as f:\n",
    "        key_text = '\\t'.join(stack_key)\n",
    "        f.write(key_text)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('2019.10': virtualenv)",
   "name": "python374jvsc74a57bd020bbdedb0079e23a85211f14a09dbcc829fefc965ff02508ebf68fe08b48d387"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "20bbdedb0079e23a85211f14a09dbcc829fefc965ff02508ebf68fe08b48d387"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}