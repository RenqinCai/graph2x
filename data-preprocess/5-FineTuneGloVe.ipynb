{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sw/centos/anaconda3/2019.10/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from nltk.corpus import brown\n",
    "from mittens import GloVe, Mittens\n",
    "from sklearn.feature_extraction import _stop_words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pickle\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove2dict(glove_filename):\n",
    "    with open(glove_filename, encoding='utf-8') as f:\n",
    "        reader = csv.reader(f, delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "        embed = {line[0]: np.array(list(map(float, line[1:])))\n",
    "                for line in reader}\n",
    "    return embed\n",
    "\n",
    "def get_rareoov(xdict, val):\n",
    "    return [k for (k,v) in Counter(xdict).items() if v<=val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 lines loaded.\n",
      "200000 lines loaded.\n",
      "300000 lines loaded.\n",
      "400000 lines loaded.\n",
      "500000 lines loaded.\n",
      "600000 lines loaded.\n",
      "700000 lines loaded.\n",
      "800000 lines loaded.\n",
      "900000 lines loaded.\n",
      "Finish loading train dataset, totally 918465 lines.\n"
     ]
    }
   ],
   "source": [
    "# Load the orignial dataset to train glove. Larger dataset can give better results for word embedding\n",
    "dir_path = '../Dataset/ratebeer/original_dataset'\n",
    "# Load train dataset\n",
    "train_review = []\n",
    "cnt = 0\n",
    "file_path = os.path.join(dir_path, 'train_no_duplicate.json')\n",
    "with open(file_path) as f:\n",
    "    for line in f:\n",
    "        line_data = json.loads(line)\n",
    "        user_id = line_data['user']\n",
    "        item_id = line_data['item']\n",
    "        rating = line_data['rating']\n",
    "        review = line_data['review']\n",
    "        train_review.append([item_id, user_id, rating, review])\n",
    "        cnt += 1\n",
    "        if cnt % 100000 == 0:\n",
    "            print('{} lines loaded.'.format(cnt))\n",
    "print('Finish loading train dataset, totally {} lines.'.format(len(train_review)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 lines loaded.\n",
      "Finish loading train dataset, totally 117818 lines.\n"
     ]
    }
   ],
   "source": [
    "dir_path = '../Dataset/ratebeer/medium_30'\n",
    "# Load train dataset\n",
    "train_review = []\n",
    "cnt = 0\n",
    "file_path = os.path.join(dir_path, 'train_review_filtered.json')\n",
    "with open(file_path) as f:\n",
    "    for line in f:\n",
    "        line_data = json.loads(line)\n",
    "        user_id = line_data['user']\n",
    "        item_id = line_data['item']\n",
    "        rating = line_data['rating']\n",
    "        review = line_data['review']\n",
    "        train_review.append([item_id, user_id, rating, review])\n",
    "        cnt += 1\n",
    "        if cnt % 100000 == 0:\n",
    "            print('{} lines loaded.'.format(cnt))\n",
    "print('Finish loading train dataset, totally {} lines.'.format(len(train_review)))\n",
    "# Load test dataset\n",
    "# test_review = []\n",
    "# cnt = 0\n",
    "# file_path = os.path.join(dir_path, 'test_review_filtered.json')\n",
    "# with open(file_path) as f:\n",
    "#     for line in f:\n",
    "#         line_data = json.loads(line)\n",
    "#         user_id = line_data['user']\n",
    "#         item_id = line_data['item']\n",
    "#         rating = line_data['rating']\n",
    "#         review = line_data['review']\n",
    "#         test_review.append([item_id, user_id, rating, review])\n",
    "#         cnt += 1\n",
    "#         if cnt % 10000 == 0:\n",
    "#             print('{} lines loaded.'.format(cnt))\n",
    "# print('Finish loading test dataset, totally {} lines.'.format(len(test_review)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data = pd.DataFrame(train_review, columns=['item', 'user', 'rating', 'review'])\n",
    "# df_test_data = pd.DataFrame(test_review, columns=['item', 'user', 'rating', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_review_doc = list(df_train_data['review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "918465"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_review_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write 918465 lines of review into file\n"
     ]
    }
   ],
   "source": [
    "with open('./embeddings/whole/train-review.txt', 'w') as f:\n",
    "    cnt = 0\n",
    "    for review in train_review_doc:\n",
    "        # write this into file\n",
    "        f.write(review.strip())\n",
    "        f.write('\\n')\n",
    "        cnt += 1\n",
    "    print('Write {} lines of review into file'.format(cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /u/pw7nc/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import simplejson\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize(infname=\"embeddings/whole/train-review.txt\"):\n",
    "    outfname = open(infname + \".tokenized\", \"w\")\n",
    "    \n",
    "    # ----------------------------------------\n",
    "    # TODO: add your code here\n",
    "    sents = open(infname).read().split(\"\\n\")\n",
    "    for sent in sents:\n",
    "        # convert characters into lower cases\n",
    "        text = sent.lower()\n",
    "        # tokenize the raw text using nltk.tokenize\n",
    "        text = word_tokenize(text)\n",
    "        # remove all punctuation\n",
    "        text = list(filter(lambda token: token not in string.punctuation, text))\n",
    "        # # replace all numbers with a special token <num>\n",
    "        # text = [re.sub('[0-9]+','<num>', token) for token in text]\n",
    "        # write this processed text into the output file\n",
    "        simplejson.dump(text, outfname)\n",
    "        outfname.write(\"\\n\")\n",
    "    # ----------------------------------------\n",
    "    outfname.close()\n",
    "\n",
    "\n",
    "def token_filter(infname=\"embeddings/whole/train-review.txt.tokenized\", thresh=5):\n",
    "    outfname = open(infname.replace(\".tokenized\", \".filtered\"), 'w')\n",
    "    vocab = []  \n",
    "    # ----------------------------------------\n",
    "    # TODO: remove \"pass\" and add your code here\n",
    "    vocab_cnt = Counter()\n",
    "    tokenized_sents = []\n",
    "    with open(infname) as f_in:\n",
    "        for line in f_in:\n",
    "            tokenized_text = simplejson.loads(line)\n",
    "            vocab_cnt.update(tokenized_text)\n",
    "            tokenized_sents.append(tokenized_text)\n",
    "    for tokenized_sent in tokenized_sents:\n",
    "        # filter word that has less than 5 TF over the whole dataset\n",
    "        filtered_text = []\n",
    "        for token in tokenized_sent:\n",
    "            if vocab_cnt[token] < thresh:\n",
    "                pass\n",
    "            else:\n",
    "                filtered_text.append(token)\n",
    "        # write this filtered data into file\n",
    "        # ignore the last line of the initial imdb-small dataset since it's an empty line\n",
    "        if len(tokenized_sent) > 0:\n",
    "            new_sentence = \" \".join(filtered_text).strip()+'\\n'\n",
    "            outfname.write(new_sentence)\n",
    "    # bulld the vocabulary\n",
    "    ## filter out token with low TF\n",
    "    vocab_cnt_filter = {token: cnt for token, cnt in vocab_cnt.items() if cnt >= thresh}\n",
    "    vocab_cnt_filter = Counter(vocab_cnt_filter)\n",
    "    ## convert this vocabulary counter to a list\n",
    "    for token,cnt in vocab_cnt_filter.most_common():\n",
    "        vocab.append(token)\n",
    "    # ----------------------------------------\n",
    "    outfname.close()\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocab size = 43670\n"
     ]
    }
   ],
   "source": [
    "tokenize()\n",
    "vocab = token_filter()\n",
    "print(\"The vocab size = {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Feature Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_2_id_file = '../Dataset/ratebeer/medium_500/train/feature/feature2id.json'\n",
    "with open(feature_2_id_file, 'r') as f:\n",
    "    feature_vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_2_id_file_large = '../Dataset/ratebeer/large/train/feature/feature2id.json'\n",
    "with open(feature_2_id_file_large, 'r') as f:\n",
    "    feature_vocab_large = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gotta\n"
     ]
    }
   ],
   "source": [
    "# Make Sure that all feature words are in the vocab\n",
    "for fea_word in list(feature_vocab_large.keys()):\n",
    "    if fea_word in vocab:\n",
    "        pass\n",
    "    else:\n",
    "        print(fea_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "5000"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_vocab_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab = set(vocab) | set(list(feature_vocab_large.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "43671"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(total_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_vocab_list = list(total_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/sw/centos/anaconda3/2019.10/lib/python3.7/site-packages/scipy/sparse/_index.py:127: SparseEfficiencyWarning: Changing the sparsity structure of a csc_matrix is expensive. lil_matrix is more efficient.\n",
      "  self._set_arrayXarray(i, j, x)\n"
     ]
    }
   ],
   "source": [
    "# sw = list(_stop_words.ENGLISH_STOP_WORDS)\n",
    "# brown_data = brown.words()[:200000]\n",
    "# brown_nonstop = [token.lower() for token in brown_data if (token.lower() not in sw)]\n",
    "\n",
    "\n",
    "\n",
    "# corp_vocab = list(set(oov))\n",
    "# brown_doc = [' '.join(brown_nonstop)]\n",
    "\n",
    "cv = CountVectorizer(ngram_range=(1,1), vocabulary=total_vocab_list)\n",
    "X = cv.fit_transform(train_review_doc)\n",
    "Xc = (X.T * X)\n",
    "Xc.setdiag(0)\n",
    "coocc_ar = Xc.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(43671, 43671)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coocc_ar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(918465, 43671)"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "43671"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vocab = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['punctuated',\n 'tails',\n '12oz',\n 'grapefriut',\n 'alaskan',\n 'transform',\n 'rivertown',\n 'farts',\n 'proably',\n 'paisse',\n 'potpouri',\n 'shits',\n 'nitrous',\n 'overides',\n 'charachter',\n 'traverse',\n 'overweight',\n 'devonshire',\n 'bubles',\n 'rockier']"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "13844"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_[\"aroma\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "43671"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vocab == total_vocab_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /sw/centos/anaconda3/2019.10/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /sw/centos/anaconda3/2019.10/lib/python3.7/site-packages/tensorflow_core/python/training/adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iteration 1000: loss: 446263.09375"
     ]
    }
   ],
   "source": [
    "from mittens import GloVe\n",
    "import numpy as np\n",
    "\n",
    "# mittens_model = Mittens(n=256, max_iter=1000)\n",
    "\n",
    "# new_embeddings = mittens_model.fit(\n",
    "#     coocc_ar,\n",
    "#     vocab=train_vocab)\n",
    "glove_model = GloVe(n=256, max_iter=1000)\n",
    "embeddings = glove_model.fit(coocc_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(43671, 256)"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "43671"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "43671"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cv.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary_words = cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "['punctuated',\n 'tails',\n '12oz',\n 'grapefriut',\n 'alaskan',\n 'transform',\n 'rivertown',\n 'farts',\n 'proably',\n 'paisse',\n 'potpouri',\n 'shits',\n 'nitrous',\n 'overides',\n 'charachter',\n 'traverse',\n 'overweight',\n 'devonshire',\n 'bubles',\n 'rockier']"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary_words[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally 43671 words begin added in the word embedding dict\n"
     ]
    }
   ],
   "source": [
    "# construct an embedding dict which has a format of {'word':'aroma', 'embedding': array([...])}\n",
    "word_embedding_dict = dict()\n",
    "for i in range(len(embeddings)):\n",
    "    current_word = vocabulary_words[i]\n",
    "    current_embedding = embeddings[i]\n",
    "    word_embedding_dict[current_word] = current_embedding\n",
    "print(\"Totally {} words begin added in the word embedding dict\".format(len(word_embedding_dict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'word_embedding_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ed25dd6c0dd8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../Dataset/ratebeer/embedding/glove.pickle'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_embedding_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHIGHEST_PROTOCOL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'word_embedding_dict' is not defined"
     ]
    }
   ],
   "source": [
    "# Save the Embedding dict into pickle file\n",
    "import pickle\n",
    "with open('../Dataset/ratebeer/embedding/glove.pickle', 'wb') as handle:\n",
    "    pickle.dump(word_embedding_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally 1000 feature words\n"
     ]
    }
   ],
   "source": [
    "# extract the embedding vector for each feature word\n",
    "feature_embedding = dict()\n",
    "emb_cnt = 0\n",
    "for feature_word in list(feature_vocab.keys()):\n",
    "    feature_word_id = cv.vocabulary_[feature_word]\n",
    "    feature_vocab_id = feature_vocab[feature_word]\n",
    "    feature_word_emb = embeddings[feature_word_id]\n",
    "    feature_embedding[feature_vocab_id] = feature_word_emb\n",
    "    emb_cnt += 1\n",
    "print(\"Totally {} feature words\".format(emb_cnt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the embedding vector for each feature word\n",
    "feature_embed_dict = dict()\n",
    "for emb_chunk in list(feature_embedding.items()):\n",
    "    feature_id = emb_chunk[0]   # this is a str\n",
    "    feature_emb_np = emb_chunk[1].tolist()\n",
    "    feature_embed_dict[feature_id] = feature_emb_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_embed_dict['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/medium_30/train/feature/featureid2embedding_wholeglove.json', 'w') as f:\n",
    "    json.dump(feature_embed_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Previously Trained Feature Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/medium_30/train/feature/featureid2embedding.json', 'r') as f:\n",
    "    feature_embed_dict_large = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "5000"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_embed_dict_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure that all the feature words in the medium feature words vocab is in the large feature word vocab\n",
    "assert len(feature_vocab.keys()) == 1000\n",
    "for feature_word_medium in feature_vocab.keys():\n",
    "    if feature_word_medium in feature_vocab_large:\n",
    "        pass\n",
    "    else:\n",
    "        print(feature_word_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the embedding vector for each feature word in the medium dataset\n",
    "feature_embed_dict_medium = dict()\n",
    "cnt = 0\n",
    "for feature_word_medium in feature_vocab.keys():\n",
    "    # get the id of the feature word\n",
    "    fea_id = feature_vocab[feature_word_medium]\n",
    "    assert fea_id == str(cnt)\n",
    "    cnt += 1\n",
    "    # get the id of the feature word in the large dataset\n",
    "    fea_id_large = feature_vocab_large[feature_word_medium]\n",
    "    assert isinstance(fea_id_large, str)\n",
    "    # get the emebdding\n",
    "    fea_embedding = feature_embed_dict_large[fea_id_large]\n",
    "    # save the feature and its embedding into the dict\n",
    "    feature_embed_dict_medium[fea_id] = fea_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1000"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_embed_dict_medium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "256"
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_embed_dict_medium['0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"../Dataset/ratebeer/medium_500/train/feature/featureid2embedding.json\", 'r') as f:\n",
    "    feature_to_embedding = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1000"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_to_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "256"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_to_embedding['999'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('2019.10': virtualenv)",
   "name": "python374jvsc74a57bd020bbdedb0079e23a85211f14a09dbcc829fefc965ff02508ebf68fe08b48d387"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "20bbdedb0079e23a85211f14a09dbcc829fefc965ff02508ebf68fe08b48d387"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}