{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sw/centos/anaconda3/2019.10/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File Description\n",
    "This file contains code that can be used to exaimine data in different domains. The code contains:\n",
    "* Convert previous __int__ ids to __str__ ids so that now all ids in each file is __str__ format\n",
    "* Check whether those sentence ids in training/testing useritem_candidate_label lies in the corresponding sentence_to_id (or id_to_sentence) mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'medium_30'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000 lines loaded.\n",
      "Finish loading train dataset, totally 117818 lines.\n",
      "10000 lines loaded.\n",
      "Finish loading test dataset, totally 14677 lines.\n"
     ]
    }
   ],
   "source": [
    "dir_path = '../Dataset/ratebeer/{}'.format(dataset_name)\n",
    "# Load train dataset\n",
    "train_review = []\n",
    "cnt = 0\n",
    "file_path = os.path.join(dir_path, 'train_review_filtered.json')\n",
    "with open(file_path) as f:\n",
    "    for line in f:\n",
    "        line_data = json.loads(line)\n",
    "        user_id = line_data['user']\n",
    "        item_id = line_data['item']\n",
    "        rating = line_data['rating']\n",
    "        review = line_data['review']\n",
    "        train_review.append([item_id, user_id, rating, review])\n",
    "        cnt += 1\n",
    "        if cnt % 100000 == 0:\n",
    "            print('{} lines loaded.'.format(cnt))\n",
    "print('Finish loading train dataset, totally {} lines.'.format(len(train_review)))\n",
    "# Load test dataset\n",
    "test_review = []\n",
    "cnt = 0\n",
    "file_path = os.path.join(dir_path, 'test_review_filtered.json')\n",
    "with open(file_path) as f:\n",
    "    for line in f:\n",
    "        line_data = json.loads(line)\n",
    "        user_id = line_data['user']\n",
    "        item_id = line_data['item']\n",
    "        rating = line_data['rating']\n",
    "        review = line_data['review']\n",
    "        test_review.append([item_id, user_id, rating, review])\n",
    "        cnt += 1\n",
    "        if cnt % 10000 == 0:\n",
    "            print('{} lines loaded.'.format(cnt))\n",
    "print('Finish loading test dataset, totally {} lines.'.format(len(test_review)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_data = pd.DataFrame(train_review, columns=['item', 'user', 'rating', 'review'])\n",
    "df_test_data = pd.DataFrame(test_review, columns=['item', 'user', 'rating', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_user_id_set = set(df_train_data['user'].unique())\n",
    "train_item_id_set = set(df_train_data['item'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence2id & id2Sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence2id and id2sentence\n",
    "with open('../Dataset/ratebeer/{}/train/sentence/sentence2id.json'.format(dataset_name), 'r') as f:\n",
    "    trainset_sent_to_id = json.load(f)\n",
    "with open('../Dataset/ratebeer/{}/train/sentence/id2sentence.json'.format(dataset_name), 'r') as f:\n",
    "    trainset_id_to_sent = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "str"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainset_sent_to_id['bottle at home .'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'easy to drink .'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_id_to_sent['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "489352"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset_id_to_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence2id and id2sentence\n",
    "with open('../Dataset/ratebeer/{}/test/sentence/sentence2id.json'.format(dataset_name), 'r') as f:\n",
    "    testset_sent_to_id = json.load(f)\n",
    "with open('../Dataset/ratebeer/{}/test/sentence/id2sentence.json'.format(dataset_name), 'r') as f:\n",
    "    testset_id_to_sent = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'bottle .'"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_id_to_sent['0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "str"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(testset_sent_to_id['bottle .'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "64334"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset_id_to_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Valid Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence2id and id2sentence\n",
    "with open('../Dataset/ratebeer/{}/valid/sentence/sentence2id.json'.format(dataset_name), 'r') as f:\n",
    "    validset_sent_to_id = json.load(f)\n",
    "with open('../Dataset/ratebeer/{}/valid/sentence/id2sentence.json'.format(dataset_name), 'r') as f:\n",
    "    validset_id_to_sent = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valid set sentence-id mapping should be identical to test set sentence-id mapping\n",
    "assert validset_id_to_sent == testset_id_to_sent\n",
    "assert validset_sent_to_id == testset_sent_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature2id and id2feature\n",
    "with open('../Dataset/ratebeer/{}/train/feature/feature2id.json'.format(dataset_name), 'r') as f:\n",
    "    feature_to_id = json.load(f)\n",
    "with open('../Dataset/ratebeer/{}/train/feature/id2feature.json'.format(dataset_name), 'r') as f:\n",
    "    id_to_feature = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "str"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(feature_to_id['aroma'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of feature on dataset medium_30: 1000\n"
     ]
    }
   ],
   "source": [
    "assert len(feature_to_id) == len(id_to_feature)\n",
    "print(\"number of feature on dataset {0}: {1}\".format(dataset_name, len(feature_to_id)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User2Feature / Item2Feature / Sentence2Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user2feature\n",
    "with open('../Dataset/ratebeer/{}/train/user/user2feature.json'.format(dataset_name), 'r') as f:\n",
    "    user_to_feature = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item2feature\n",
    "with open('../Dataset/ratebeer/{}/train/item/item2feature.json'.format(dataset_name), 'r') as f:\n",
    "    item_to_feature = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence2feature\n",
    "with open('../Dataset/ratebeer/{}/train/sentence/sentence2feature.json'.format(dataset_name), 'r') as f:\n",
    "    sentence_to_feature = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On dataset: medium_30\n",
      "number of user on train set: 1664\n",
      "number of item on train set: 1490\n",
      "number of sentence on train set: 489352\n",
      "=========================================\n",
      "max number of feature per user: 849\n",
      "min number of feature per user: 14\n",
      "mean number of feature per user: 325.80528846153845\n",
      "=========================================\n",
      "max number of feature per item: 760\n",
      "min number of feature per item: 157\n",
      "mean number of feature per item: 398.403355704698\n",
      "=========================================\n",
      "max number of feature per sentence: 40\n",
      "min number of feature per sentence: 1\n",
      "mean number of feature per sentence: 5.371963331099086\n"
     ]
    }
   ],
   "source": [
    "user_to_feature_cnt = []\n",
    "item_to_feature_cnt = []\n",
    "sentence_to_feature_cnt = []\n",
    "for key,value in user_to_feature.items():\n",
    "    user_to_feature_cnt.append(len(value))\n",
    "for key,value in item_to_feature.items():\n",
    "    item_to_feature_cnt.append(len(value))\n",
    "for key,value in sentence_to_feature.items():\n",
    "    sentence_to_feature_cnt.append(len(value))\n",
    "\n",
    "print(\"On dataset: {}\".format(dataset_name))\n",
    "print(\"number of user on train set: {}\".format(len(user_to_feature_cnt)))\n",
    "print(\"number of item on train set: {}\".format(len(item_to_feature_cnt)))\n",
    "print(\"number of sentence on train set: {}\".format(len(sentence_to_feature_cnt)))\n",
    "print(\"=========================================\")\n",
    "print(\"max number of feature per user: {}\".format(np.max(user_to_feature_cnt)))\n",
    "print(\"min number of feature per user: {}\".format(np.min(user_to_feature_cnt)))\n",
    "print(\"mean number of feature per user: {}\".format(np.mean(user_to_feature_cnt)))\n",
    "print(\"=========================================\")\n",
    "print(\"max number of feature per item: {}\".format(np.max(item_to_feature_cnt)))\n",
    "print(\"min number of feature per item: {}\".format(np.min(item_to_feature_cnt)))\n",
    "print(\"mean number of feature per item: {}\".format(np.mean(item_to_feature_cnt)))\n",
    "print(\"=========================================\")\n",
    "print(\"max number of feature per sentence: {}\".format(np.max(sentence_to_feature_cnt)))\n",
    "print(\"min number of feature per sentence: {}\".format(np.min(sentence_to_feature_cnt)))\n",
    "print(\"mean number of feature per sentence: {}\".format(np.mean(sentence_to_feature_cnt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UserItem2Sentids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issue with the training set\n",
    "Due to the selection of training sentences, we omitted sentence with less than 3 effective tokens. This may lead to some review's sentences are all being omiited which then lead to an __empty__ true label sentence id list.\n",
    "\n",
    "This issue is now __FIXED__. Data which contains empty true label sentence id list has been removed from the training set (i.e. *useritem_to_sentid* )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "empty_label = 0\n",
    "for user_chunk in list(useritem_to_sentids.items()):\n",
    "    user_id_str = str(user_chunk[0])\n",
    "    # print(user_id_str)\n",
    "    user_item_chunks = list(user_chunk[1].items())\n",
    "    for item_chunk in user_item_chunks:\n",
    "        item_id_str = str(item_chunk[0])\n",
    "        # print(item_id_str)\n",
    "        # candidate_true_sent_ids = item_chunk[1]\n",
    "        candidate_sent_ids = item_chunk[1][0]\n",
    "        true_sent_ids = item_chunk[1][1]\n",
    "        if len(true_sent_ids) == 0:\n",
    "            # print(item_chunk)\n",
    "            empty_label += 1\n",
    "print(empty_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_chunk in list(useritem_to_sentids.items()):\n",
    "    user_id_str = str(user_chunk[0])\n",
    "    # print(user_id_str)\n",
    "    user_item_chunks = list(user_chunk[1].items())\n",
    "    # construct a item-level dict\n",
    "    item_level_dict = dict()\n",
    "    for item_chunk in user_item_chunks:\n",
    "        item_id_str = str(item_chunk[0])\n",
    "        # print(item_id_str)\n",
    "        # candidate_true_sent_ids = item_chunk[1]\n",
    "        candidate_sent_ids = item_chunk[1][0]\n",
    "        true_sent_ids = item_chunk[1][1]\n",
    "        candidate_sent_ids_str = list(map(str, candidate_sent_ids))\n",
    "        true_sent_ids_str = list(map(str, true_sent_ids))\n",
    "        candidate_true_sent_ids_str = [candidate_sent_ids_str, true_sent_ids_str]\n",
    "        if len(true_sent_ids_str) > 0:\n",
    "            # add this into item-level dict\n",
    "            item_level_dict[item_id_str] = candidate_true_sent_ids_str\n",
    "    # update user-level dict\n",
    "    useritem_to_sentids[user_id_str] = item_level_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_chunk in list(useritem_to_sentids.items()):\n",
    "    user_id_str = str(user_chunk[0])\n",
    "    # print(user_id_str)\n",
    "    user_item_chunks = list(user_chunk[1].items())\n",
    "    for item_chunk in user_item_chunks:\n",
    "        item_id_str = str(item_chunk[0])\n",
    "        # print(item_id_str)\n",
    "        # candidate_true_sent_ids = item_chunk[1]\n",
    "        candidate_sent_ids = item_chunk[1][0]\n",
    "        true_sent_ids = item_chunk[1][1]\n",
    "        assert isinstance(candidate_sent_ids[0], str)\n",
    "        assert isinstance(true_sent_ids[0], str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write file\n",
    "# with open('../Dataset/ratebeer/train/useritem2sentids.json', 'w') as f:\n",
    "#     json.dump(useritem_to_sentids, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1664"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(useritem_to_sentids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1490\n"
     ]
    }
   ],
   "source": [
    "itemset = set()\n",
    "for user_chunk in list(useritem_to_sentids.items()):\n",
    "    user_id_str = str(user_chunk[0])\n",
    "    # print(user_id_str)\n",
    "    user_item_chunks = list(user_chunk[1].items())\n",
    "    for item_chunk in user_item_chunks:\n",
    "        item_id_str = str(item_chunk[0])\n",
    "        itemset.add(item_id_str)\n",
    "print(len(itemset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UserItem2SentIDs on Testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user2feature\n",
    "with open('../Dataset/ratebeer/medium_30/test/useritem2sentids_test.json', 'r') as f:\n",
    "    testset_useritem_to_sentids = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1664"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset_useritem_to_sentids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "empty_label = 0\n",
    "for user_chunk in list(testset_useritem_to_sentids.items()):\n",
    "    user_id_str = str(user_chunk[0])\n",
    "    # print(user_id_str)\n",
    "    user_item_chunks = list(user_chunk[1].items())\n",
    "    for item_chunk in user_item_chunks:\n",
    "        item_id_str = str(item_chunk[0])\n",
    "        # print(item_id_str)\n",
    "        # candidate_true_sent_ids = item_chunk[1]\n",
    "        candidate_sent_ids = item_chunk[1][0]\n",
    "        true_sent_ids = item_chunk[1][1]\n",
    "        if len(true_sent_ids) == 0:\n",
    "            # print(item_chunk)\n",
    "            empty_label += 1\n",
    "print(empty_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1483\n"
     ]
    }
   ],
   "source": [
    "test_itemset = set()\n",
    "for user_chunk in list(testset_useritem_to_sentids.items()):\n",
    "    user_id_str = str(user_chunk[0])\n",
    "    # print(user_id_str)\n",
    "    user_item_chunks = list(user_chunk[1].items())\n",
    "    for item_chunk in user_item_chunks:\n",
    "        item_id_str = str(item_chunk[0])\n",
    "        test_itemset.add(item_id_str)\n",
    "print(len(test_itemset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for test_item_id in test_itemset:\n",
    "    assert test_item_id in itemset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_chunk in list(testset_useritem_to_sentids.items()):\n",
    "    user_id_str = str(user_chunk[0])\n",
    "    # print(user_id_str)\n",
    "    user_item_chunks = list(user_chunk[1].items())\n",
    "    # construct a item-level dict\n",
    "    item_level_dict = dict()\n",
    "    for item_chunk in user_item_chunks:\n",
    "        item_id_str = str(item_chunk[0])\n",
    "        # print(item_id_str)\n",
    "        # candidate_true_sent_ids = item_chunk[1]\n",
    "        candidate_sent_ids = item_chunk[1][0]\n",
    "        true_sent_ids = item_chunk[1][1]\n",
    "        candidate_sent_ids_str = list(map(str, candidate_sent_ids))\n",
    "        true_sent_ids_str = list(map(str, true_sent_ids))\n",
    "        candidate_true_sent_ids_str = [candidate_sent_ids_str, true_sent_ids_str]\n",
    "        if len(true_sent_ids_str) > 0:\n",
    "            # add this into item-level dict\n",
    "            item_level_dict[item_id_str] = candidate_true_sent_ids_str\n",
    "    # update user-level dict\n",
    "    testset_useritem_to_sentids[user_id_str] = item_level_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user_chunk in list(testset_useritem_to_sentids.items()):\n",
    "    user_id_str = str(user_chunk[0])\n",
    "    # print(user_id_str)\n",
    "    user_item_chunks = list(user_chunk[1].items())\n",
    "    for item_chunk in user_item_chunks:\n",
    "        item_id_str = str(item_chunk[0])\n",
    "        # print(item_id_str)\n",
    "        # candidate_true_sent_ids = item_chunk[1]\n",
    "        candidate_sent_ids = item_chunk[1][0]\n",
    "        true_sent_ids = item_chunk[1][1]\n",
    "        assert isinstance(candidate_sent_ids[0], str)\n",
    "        assert isinstance(true_sent_ids[0], str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # write file\n",
    "# with open('../Dataset/ratebeer/test/useritem2sentids_test.json', 'w') as f:\n",
    "#     json.dump(testset_useritem_to_sentids, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Whether the true sentence ids are in the test id2sentence mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1664"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset_useritem_to_sentids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/medium_30/test/useritem2sentids_test.json', 'r') as f:\n",
    "    testset_useritem_to_sentids_load = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset_useritem_to_sentids == testset_useritem_to_sentids_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_set_sent_ids = set()\n",
    "number_of_sentence = 0\n",
    "for user_chunk in list(testset_useritem_to_sentids.items()):\n",
    "    user_id_str = user_chunk[0]\n",
    "    assert isinstance(user_id_str, str)\n",
    "    user_item_chunks = list(user_chunk[1].items())\n",
    "    for item_chunk in user_item_chunks:\n",
    "        item_id_str = item_chunk[0]\n",
    "        assert isinstance(item_id_str, str)\n",
    "        candidate_sent_ids = item_chunk[1][0]\n",
    "        true_sent_ids = item_chunk[1][1]\n",
    "        assert isinstance(candidate_sent_ids[0], str)\n",
    "        assert isinstance(true_sent_ids[0], str)\n",
    "        # check if all the true sent ids lies in the testset_id_to_sent mapping\n",
    "        for cur_true_sent_id in true_sent_ids:\n",
    "            # add this sent id into set\n",
    "            true_set_sent_ids.add(cur_true_sent_id)\n",
    "            number_of_sentence += 1\n",
    "            if cur_true_sent_id not in testset_id_to_sent:\n",
    "                print(cur_true_sent_id)\n",
    "                print(item_chunk)\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "64334"
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_set_sent_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "68066"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('../Dataset/ratebeer/train/feature/featureidembedding.pickle', 'rb') as handle:\n",
    "    feature_emb = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "dict"
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(feature_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the embedding vector for each feature word\n",
    "feature_embed_dict = dict()\n",
    "for emb_chunk in list(feature_emb.items()):\n",
    "    feature_id = emb_chunk[0]\n",
    "    feature_emb_np = emb_chunk[1].tolist()\n",
    "    feature_embed_dict[feature_id] = feature_emb_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/train/feature/featureid2embedding.json', 'w') as f:\n",
    "    json.dump(feature_embed_dict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Train/Test data (useritem2senids) into multiple lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# useritem2sentenceid\n",
    "with open('../Dataset/ratebeer/train/useritem2sentids.json', 'r') as f:\n",
    "    trainset_useritem_to_sentids = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2963"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset_useritem_to_sentids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'1000'"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(trainset_useritem_to_sentids.items())[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "dict"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(list(trainset_useritem_to_sentids.items())[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews: 302344\n",
      "Total number of user: 2963\n",
      "Total number of item: 3744\n"
     ]
    }
   ],
   "source": [
    "# Checking How Many User/Item/Review in the training set\n",
    "cnt_user = 0\n",
    "cnt_review = 0\n",
    "cnt_item_set = set()\n",
    "for trainset_user_chunk in list(trainset_useritem_to_sentids.items()):\n",
    "    user_id_str = str(trainset_user_chunk[0])\n",
    "    user_id = int(trainset_user_chunk[0])\n",
    "    user_item_chunks = list(trainset_user_chunk[1].items())\n",
    "    for item_chunk in user_item_chunks:\n",
    "        item_id_str = str(item_chunk[0])\n",
    "        item_id = int(item_chunk[0])\n",
    "        # candidate_true_sent_ids = item_chunk[1]\n",
    "        # cur_data_dict = {'user_id': user_id, 'item_id': item_id, 'sent_id': candidate_true_sent_ids}\n",
    "        # write this into the json file\n",
    "        # json.dump(cur_data_dict, f1)\n",
    "        # f1.write(\"\\n\")\n",
    "        assert user_id_str in train_user_id_set\n",
    "        assert item_id_str in train_item_id_set\n",
    "        cnt_item_set.add(item_id_str)\n",
    "        cnt_review += 1\n",
    "    cnt_user += 1\n",
    "\n",
    "print(\"Total number of reviews: {}\".format(cnt_review))\n",
    "print(\"Total number of user: {}\".format(cnt_user))\n",
    "print(\"Total number of item: {}\".format(len(cnt_item_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 2963 users\n",
      "Totat 302344 reviews\n"
     ]
    }
   ],
   "source": [
    "# Write useritem2sentids into a line-by-line format\n",
    "with open('../Dataset/ratebeer/train/useritem2sentids_multilines.json', 'a') as f1:\n",
    "    cnt_user = 0\n",
    "    cnt_review = 0\n",
    "    user_set = set()\n",
    "    useritem_set = set()\n",
    "    for trainset_user_chunk in list(trainset_useritem_to_sentids.items()):\n",
    "        user_id_str = str(trainset_user_chunk[0])\n",
    "        user_id = int(trainset_user_chunk[0])\n",
    "        user_item_chunks = list(trainset_user_chunk[1].items())\n",
    "        for item_chunk in user_item_chunks:\n",
    "            item_id_str = str(item_chunk[0])\n",
    "            item_id = int(item_chunk[0])\n",
    "            candidate_sent_ids = item_chunk[1][0]\n",
    "            gold_revw_sent_ids = item_chunk[1][1]\n",
    "            cur_data_dict = {'user_id': user_id, 'item_id': item_id, 'candidate': candidate_sent_ids, \"review\": gold_revw_sent_ids}\n",
    "            # write this into the json file\n",
    "            json.dump(cur_data_dict, f1)\n",
    "            f1.write(\"\\n\")\n",
    "            cnt_review += 1\n",
    "            useritem_set.add((user_id_str, item_id_str))\n",
    "        cnt_user += 1\n",
    "        user_set.add(user_id_str)\n",
    "\n",
    "print(\"Total {} users\".format(cnt_user))\n",
    "print(\"Totat {} reviews\".format(cnt_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "302344"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(useritem_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# useritem2sentenceid\n",
    "with open('../Dataset/ratebeer/medium_30/test/useritem2sentids_test.json', 'r') as f:\n",
    "    testset_useritem_to_sentids = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1664"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset_useritem_to_sentids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews: 14675\n",
      "Total number of user: 1664\n",
      "Total number of item: 1483\n"
     ]
    }
   ],
   "source": [
    "# Checking How Many User/Item/Review are in the test set\n",
    "cnt_user = 0\n",
    "cnt_review = 0\n",
    "cnt_item_set = set()\n",
    "for trainset_user_chunk in list(testset_useritem_to_sentids.items()):\n",
    "    user_id_str = str(trainset_user_chunk[0])\n",
    "    user_id = int(trainset_user_chunk[0])\n",
    "    user_item_chunks = list(trainset_user_chunk[1].items())\n",
    "    for item_chunk in user_item_chunks:\n",
    "        item_id_str = str(item_chunk[0])\n",
    "        item_id = int(item_chunk[0])\n",
    "        # candidate_true_sent_ids = item_chunk[1]\n",
    "        # cur_data_dict = {'user_id': user_id, 'item_id': item_id, 'sent_id': candidate_true_sent_ids}\n",
    "        # write this into the json file\n",
    "        # json.dump(cur_data_dict, f1)\n",
    "        # f1.write(\"\\n\")\n",
    "        assert user_id_str in train_user_id_set\n",
    "        assert item_id_str in train_item_id_set\n",
    "        cnt_item_set.add(item_id_str)\n",
    "        cnt_review += 1\n",
    "    cnt_user += 1\n",
    "\n",
    "print(\"Total number of reviews: {}\".format(cnt_review))\n",
    "print(\"Total number of user: {}\".format(cnt_user))\n",
    "print(\"Total number of item: {}\".format(len(cnt_item_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 1664 users\n",
      "Totat 14675 reviews\n"
     ]
    }
   ],
   "source": [
    "# Write useritem2sentids_test into a line-by-line format\n",
    "with open('../Dataset/ratebeer/medium_30/test/useritem2sentids_test_multilines.json', 'a') as f1:\n",
    "    cnt_user = 0\n",
    "    cnt_review = 0\n",
    "    user_set = set()\n",
    "    useritem_set = set()\n",
    "    for trainset_user_chunk in list(testset_useritem_to_sentids.items()):\n",
    "        user_id_str = str(trainset_user_chunk[0])\n",
    "        user_id = int(trainset_user_chunk[0])\n",
    "        user_item_chunks = list(trainset_user_chunk[1].items())\n",
    "        for item_chunk in user_item_chunks:\n",
    "            item_id_str = str(item_chunk[0])\n",
    "            item_id = int(item_chunk[0])\n",
    "            candidate_sent_ids = item_chunk[1][0]\n",
    "            true_revw_sent_ids = item_chunk[1][1]\n",
    "            cur_data_dict = {'user_id':user_id, 'item_id':item_id, 'candidate':candidate_sent_ids, \"review\":true_revw_sent_ids}\n",
    "            # write this into the json file\n",
    "            json.dump(cur_data_dict, f1)\n",
    "            f1.write(\"\\n\")\n",
    "            cnt_review += 1\n",
    "            useritem_set.add((user_id_str, item_id_str))\n",
    "        cnt_user += 1\n",
    "        user_set.add(user_id_str)\n",
    "\n",
    "print(\"Total {} users\".format(cnt_user))\n",
    "print(\"Totat {} reviews\".format(cnt_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "14675"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(useritem_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'would not ever want to drink again .'"
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_sent['335048']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'nice spicy taste with some yeast in the finish great beer'"
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_sent['334999']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'good aroma , had a few sips of anothers beer and liked it .'"
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_sent['335017']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'i was hoping for more flavor than what i got .'"
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_sent['335005']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User2Sentid and Item2Sentid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user2sentid\n",
    "with open('../Dataset/ratebeer/train/user/user2sentids.json', 'r') as f:\n",
    "    trainset_user_to_sentids = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item2sentid\n",
    "with open('../Dataset/ratebeer/train/item/item2sentids.json', 'r') as f:\n",
    "    trainset_item_to_sentids = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2963"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(trainset_user_to_sentids.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3744"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(trainset_item_to_sentids.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of sentences per user: 437.58825514681064\n",
      "Max number of sentences per user: 4398\n",
      "Min number of sentences per user: 8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "user_num_sentences_cnt = []\n",
    "for key,value in trainset_user_to_sentids.items():\n",
    "    user_num_sentences_cnt.append(len(value))\n",
    "print(\"Mean number of sentences per user: {}\".format(np.mean(user_num_sentences_cnt)))\n",
    "print(\"Max number of sentences per user: {}\".format(np.max(user_num_sentences_cnt)))\n",
    "print(\"Min number of sentences per user: {}\".format(np.min(user_num_sentences_cnt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of sentences per item: 355.7083333333333\n",
      "Max number of sentences per item: 2774\n",
      "Min number of sentences per item: 49\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "item_num_sentences_cnt = []\n",
    "for key,value in trainset_item_to_sentids.items():\n",
    "    item_num_sentences_cnt.append(len(value))\n",
    "print(\"Mean number of sentences per item: {}\".format(np.mean(item_num_sentences_cnt)))\n",
    "print(\"Max number of sentences per item: {}\".format(np.max(item_num_sentences_cnt)))\n",
    "print(\"Min number of sentences per item: {}\".format(np.min(item_num_sentences_cnt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UserItemCandidateSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useritem2sentenceid\n",
    "with open('../Dataset/ratebeer/train/useritem2sentids.json', 'r') as f:\n",
    "    trainset_useritem_to_sentids = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of users: 2963\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of users: {}\".format(len(trainset_useritem_to_sentids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally 302344 reviews.\n"
     ]
    }
   ],
   "source": [
    "trainset_user_num_sentences_cnt = []        # number of sentences relevant to each user on trainset\n",
    "trainset_item_num_sentences_cnt = []        # number of sentences relevant to each item on trainset\n",
    "trainset_review_num_sentences_cnt = []      # number of sentences in true review set in each line of trainset\n",
    "trainset_candidate_num_sentences_cnt = []   # number of sentecnes in candidate set in each line of trainset\n",
    "\n",
    "trainset_user_sentids_set = dict()\n",
    "trainset_item_sentids_set = dict()\n",
    "\n",
    "cnt_reviews = 0\n",
    "\n",
    "for user_id, user_chunks in trainset_useritem_to_sentids.items():\n",
    "    for item_id, user_item_chunk in user_chunks.items():\n",
    "        candidate_sents = user_item_chunk[0]\n",
    "        gold_revw_sents = user_item_chunk[1]\n",
    "        # add number of review sentences\n",
    "        trainset_review_num_sentences_cnt.append(len(gold_revw_sents))\n",
    "        # add number of candidate sentences\n",
    "        trainset_candidate_num_sentences_cnt.append(len(candidate_sents))\n",
    "        # add candidate sents to user\n",
    "        if user_id in trainset_user_sentids_set:\n",
    "            trainset_user_sentids_set[user_id].update(candidate_sents)\n",
    "        else:\n",
    "            trainset_user_sentids_set[user_id] = set(candidate_sents)\n",
    "        # add candidate sents to item\n",
    "        if item_id in trainset_item_sentids_set:\n",
    "            trainset_item_sentids_set[item_id].update(candidate_sents)\n",
    "        else:\n",
    "            trainset_item_sentids_set[item_id] = set(candidate_sents)\n",
    "        # count this review\n",
    "        cnt_reviews += 1\n",
    "print(\"Totally {} reviews.\".format(cnt_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of sentences relevant to each user in trainset: 23027.967262909213\n",
      "Max number of sentences relevant to each user in trainset: 98865\n",
      "Min number of sentences relevant to each user in trainset: 988\n",
      "\n",
      "\n",
      "Mean number of sentences relevant to each item in trainset: 20221.916666666668\n",
      "Max number of sentences relevant to each item in trainset: 60410\n",
      "Min number of sentences relevant to each item in trainset: 5865\n"
     ]
    }
   ],
   "source": [
    "for key, value in trainset_user_sentids_set.items():\n",
    "    trainset_user_num_sentences_cnt.append(len(value))\n",
    "for key, value in trainset_item_sentids_set.items():\n",
    "    trainset_item_num_sentences_cnt.append(len(value))\n",
    "\n",
    "# some statistics\n",
    "print(\"Mean number of sentences relevant to each user in trainset: {}\".format(np.mean(trainset_user_num_sentences_cnt)))\n",
    "print(\"Max number of sentences relevant to each user in trainset: {}\".format(np.max(trainset_user_num_sentences_cnt)))\n",
    "print(\"Min number of sentences relevant to each user in trainset: {}\".format(np.min(trainset_user_num_sentences_cnt)))\n",
    "print(\"\\n\")\n",
    "print(\"Mean number of sentences relevant to each item in trainset: {}\".format(np.mean(trainset_item_num_sentences_cnt)))\n",
    "print(\"Max number of sentences relevant to each item in trainset: {}\".format(np.max(trainset_item_num_sentences_cnt)))\n",
    "print(\"Min number of sentences relevant to each item in trainset: {}\".format(np.min(trainset_item_num_sentences_cnt)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean number of sentences per candidate-set in trainset: 475.1722144312439\n",
      "Max number of sentences per candidate-set in trainset: 546\n",
      "Min number of sentences per candidate-set in trainset: 8\n",
      "\n",
      "\n",
      "Mean number of sentences per true review in trainset: 4.419161617230704\n",
      "Max number of sentences per true review in trainset: 49\n",
      "Min number of sentences per true review in trainset: 1\n"
     ]
    }
   ],
   "source": [
    "# some statistics\n",
    "print(\"Mean number of sentences per candidate-set in trainset: {}\".format(np.mean(trainset_candidate_num_sentences_cnt)))\n",
    "print(\"Max number of sentences per candidate-set in trainset: {}\".format(np.max(trainset_candidate_num_sentences_cnt)))\n",
    "print(\"Min number of sentences per candidate-set in trainset: {}\".format(np.min(trainset_candidate_num_sentences_cnt)))\n",
    "print(\"\\n\")\n",
    "print(\"Mean number of sentences per true review in trainset: {}\".format(np.mean(trainset_review_num_sentences_cnt)))\n",
    "print(\"Max number of sentences per true review in trainset: {}\".format(np.max(trainset_review_num_sentences_cnt)))\n",
    "print(\"Min number of sentences per true review in trainset: {}\".format(np.min(trainset_review_num_sentences_cnt)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python374jvsc74a57bd020bbdedb0079e23a85211f14a09dbcc829fefc965ff02508ebf68fe08b48d387"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "20bbdedb0079e23a85211f14a09dbcc829fefc965ff02508ebf68fe08b48d387"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}