{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/sw/centos/anaconda3/2019.10/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfTransformer\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "nlp = English()\n",
    "# Create a Tokenizer with the default settings for English\n",
    "# including punctuation rules and exceptions\n",
    "tokenizer = nlp.tokenizer\n",
    "import string\n",
    "punct = string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = 'large_500'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 lines loaded.\n",
      "20000 lines loaded.\n",
      "30000 lines loaded.\n",
      "40000 lines loaded.\n",
      "Finish loading test dataset, totally 40730 lines.\n"
     ]
    }
   ],
   "source": [
    "dir_path = '../Dataset/ratebeer/{}'.format(dataset_name)\n",
    "# Load test dataset\n",
    "test_review = []\n",
    "cnt = 0\n",
    "file_path = os.path.join(dir_path, 'test_review_filtered.json')\n",
    "with open(file_path) as f:\n",
    "    for line in f:\n",
    "        line_data = json.loads(line)\n",
    "        user_id = line_data['user']\n",
    "        item_id = line_data['item']\n",
    "        rating = line_data['rating']\n",
    "        review = line_data['review']\n",
    "        test_review.append([item_id, user_id, rating, review])\n",
    "        cnt += 1\n",
    "        if cnt % 10000 == 0:\n",
    "            print('{} lines loaded.'.format(cnt))\n",
    "print('Finish loading test dataset, totally {} lines.'.format(len(test_review)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_data = pd.DataFrame(test_review, columns=['item', 'user', 'rating', 'review'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>item</th>\n      <th>user</th>\n      <th>rating</th>\n      <th>review</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>760</td>\n      <td>999</td>\n      <td>14</td>\n      <td>bomber pours a hazy caramel hued body with a s...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>842</td>\n      <td>999</td>\n      <td>14</td>\n      <td>bottle pours ahazy apricot body with a small o...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>442</td>\n      <td>999</td>\n      <td>16</td>\n      <td>pours a hazy , dark caramel body with no head ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>274</td>\n      <td>999</td>\n      <td>16</td>\n      <td>hazed peach body supports a small offwhite hea...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>476</td>\n      <td>999</td>\n      <td>15</td>\n      <td>picked up a couple bottles of these at a local...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>40725</th>\n      <td>325</td>\n      <td>3849</td>\n      <td>1</td>\n      <td>this beer is not worth my time to rate it , so...</td>\n    </tr>\n    <tr>\n      <th>40726</th>\n      <td>446</td>\n      <td>3849</td>\n      <td>5</td>\n      <td>really not terrible for the price , but has th...</td>\n    </tr>\n    <tr>\n      <th>40727</th>\n      <td>538</td>\n      <td>3849</td>\n      <td>12</td>\n      <td>now this shit here is my bread and butter . ol...</td>\n    </tr>\n    <tr>\n      <th>40728</th>\n      <td>571</td>\n      <td>3849</td>\n      <td>2</td>\n      <td>well , colt 45 earns credit for at least one t...</td>\n    </tr>\n    <tr>\n      <th>40729</th>\n      <td>2156</td>\n      <td>3849</td>\n      <td>7</td>\n      <td>the thing about stock though , i think it is a...</td>\n    </tr>\n  </tbody>\n</table>\n<p>40730 rows Ã— 4 columns</p>\n</div>",
      "text/plain": "       item  user  rating                                             review\n0       760   999      14  bomber pours a hazy caramel hued body with a s...\n1       842   999      14  bottle pours ahazy apricot body with a small o...\n2       442   999      16  pours a hazy , dark caramel body with no head ...\n3       274   999      16  hazed peach body supports a small offwhite hea...\n4       476   999      15  picked up a couple bottles of these at a local...\n...     ...   ...     ...                                                ...\n40725   325  3849       1  this beer is not worth my time to rate it , so...\n40726   446  3849       5  really not terrible for the price , but has th...\n40727   538  3849      12  now this shit here is my bread and butter . ol...\n40728   571  3849       2  well , colt 45 earns credit for at least one t...\n40729  2156  3849       7  the thing about stock though , i think it is a...\n\n[40730 rows x 4 columns]"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sentence2ID and ID2Sentence Mapping From Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/{}/train/sentence/sentence2id.json'.format(dataset_name), 'r') as f:\n",
    "    trainset_sent_to_id = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "str"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trainset_sent_to_id['bottle at home .'])\n",
    "# the id here is str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/{}/train/sentence/id2sentence.json'.format(dataset_name), 'r') as f:\n",
    "    trainset_id_to_sent = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'bottle at home .'"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset_id_to_sent['0']\n",
    "# the id here is str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1246458 sentences in the training set.\n"
     ]
    }
   ],
   "source": [
    "assert len(trainset_sent_to_id) == len(trainset_id_to_sent)\n",
    "print(\"There are {} sentences in the training set.\".format(len(trainset_id_to_sent)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Feature Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature words are the same between training and testing\n",
    "# since we can only know the review text from training set\n",
    "feature_2_id_file = '../Dataset/ratebeer/{}/train/feature/feature2id.json'.format(dataset_name)\n",
    "with open(feature_2_id_file, 'r') as f:\n",
    "    feature_vocab = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2000"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of feature words: 2000\n"
     ]
    }
   ],
   "source": [
    "feature_word_list = list(feature_vocab.keys())\n",
    "print('Number of feature words: {}'.format(len(feature_word_list)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sentence2Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Sentence Vocab on Test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Whether there are reviews with no sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "invalid_data = 0\n",
    "for idx, row in df_test_data.iterrows():\n",
    "    review_text = row['review']\n",
    "    review_sents = sent_tokenize(review_text)\n",
    "    if len(review_sents) == 0:\n",
    "        print(row)\n",
    "        invalid_data += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(invalid_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_score(text, feature_word_list):\n",
    "    vectorizer = CountVectorizer(lowercase=True, vocabulary=feature_word_list)\n",
    "    word_count = vectorizer.fit_transform(text)\n",
    "    return word_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000 lines\n",
      "Processed 2000 lines\n",
      "Processed 3000 lines\n",
      "Processed 4000 lines\n",
      "Processed 5000 lines\n",
      "Processed 6000 lines\n",
      "Processed 7000 lines\n",
      "Processed 8000 lines\n",
      "Processed 9000 lines\n",
      "Processed 10000 lines\n",
      "Processed 11000 lines\n",
      "Processed 12000 lines\n",
      "Processed 13000 lines\n",
      "Processed 14000 lines\n",
      "Processed 15000 lines\n",
      "Processed 16000 lines\n",
      "Processed 17000 lines\n",
      "Processed 18000 lines\n",
      "Processed 19000 lines\n",
      "Processed 20000 lines\n",
      "Processed 21000 lines\n",
      "Processed 22000 lines\n",
      "Processed 23000 lines\n",
      "Processed 24000 lines\n",
      "Processed 25000 lines\n",
      "Processed 26000 lines\n",
      "Processed 27000 lines\n",
      "Processed 28000 lines\n",
      "Processed 29000 lines\n",
      "Processed 30000 lines\n",
      "Processed 31000 lines\n",
      "Processed 32000 lines\n",
      "Processed 33000 lines\n",
      "Processed 34000 lines\n",
      "Processed 35000 lines\n",
      "Processed 36000 lines\n",
      "Processed 37000 lines\n",
      "Processed 38000 lines\n",
      "Processed 39000 lines\n",
      "Processed 40000 lines\n",
      "There are 506 sentences with no feature words\n"
     ]
    }
   ],
   "source": [
    "# sentence vocab\n",
    "sentence_count = dict()\n",
    "sentence_with_no_feature = 0\n",
    "# Loop for each review\n",
    "for idx, row in df_test_data.iterrows():\n",
    "    review_text = row['review']\n",
    "    review_sents = sent_tokenize(review_text)\n",
    "    tf_score = get_tf_score(review_sents, feature_word_list)\n",
    "    tf_sum_sents = np.sum(tf_score, axis=1)\n",
    "    for i in range(len(review_sents)):\n",
    "        if tf_sum_sents[i] != 0.0:\n",
    "            cur_sent = review_sents[i]\n",
    "            sentence_count[cur_sent] = 1 + sentence_count.get(cur_sent, 0)\n",
    "        else:\n",
    "            sentence_with_no_feature += 1\n",
    "    if (idx+1) % 1000 == 0:\n",
    "        print(\"Processed {} lines\".format(idx+1))\n",
    "print(\"There are {} sentences with no feature words\".format(sentence_with_no_feature))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "176015"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort sentence based on counts (the majority should be 1)\n",
    "sorted_sent_counts = sorted(sentence_count.items(), key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentence_vocab_list = list(sentence_count.keys())\n",
    "# Building mappings from sentences to ids and ids to sentences\n",
    "testset_sent_to_id = {entry[0]: str(id) for (id, entry) in enumerate(sorted_sent_counts)}\n",
    "# Since we loaded all the tokenized sentences, we don't need to add the special UNK token\n",
    "testset_id_to_sent = {str(id): sent for (sent, id) in testset_sent_to_id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Sentence to ID into Json File (Test set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/{}/test/sentence/id2sentence.json'.format(dataset_name), 'w') as f:\n",
    "    json.dump(testset_id_to_sent, f)\n",
    "\n",
    "with open('../Dataset/ratebeer/{}/test/sentence/sentence2id.json'.format(dataset_name), 'w') as f:\n",
    "    json.dump(testset_sent_to_id, f)\n",
    "\n",
    "with open('../Dataset/ratebeer/{}/valid/sentence/id2sentence.json'.format(dataset_name), 'w') as f:\n",
    "    json.dump(testset_id_to_sent, f)\n",
    "\n",
    "with open('../Dataset/ratebeer/{}/valid/sentence/sentence2id.json'.format(dataset_name), 'w') as f:\n",
    "    json.dump(testset_sent_to_id, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/{}/valid/sentence/id2sentence.json'.format(dataset_name), 'r') as f:\n",
    "    validset_id_to_sent = json.load(f)\n",
    "\n",
    "with open('../Dataset/ratebeer/{}/valid/sentence/sentence2id.json'.format(dataset_name), 'r') as f:\n",
    "    validset_sent_to_id = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "176015"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(testset_id_to_sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load User to Sentence ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/{}/train/user/user2sentids.json'.format(dataset_name), 'r') as f:\n",
    "    trainset_user_to_sent_id = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Item to Sentence ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../Dataset/ratebeer/{}/train/item/item2sentids.json'.format(dataset_name), 'r') as f:\n",
    "    trainset_item_to_sent_id = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2963"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset_user_to_sent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "140"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset_user_to_sent_id['1223'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "3744"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset_item_to_sent_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1954"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset_item_to_sent_id['199'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Each Data Instance in Testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_by_user_test = df_test_data.groupby('user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2963"
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(group_by_user_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2963"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(group_by_user_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Valid Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember in valid set we are doing sampling as what the we did on the train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 user processed.\n",
      "1000 user processed.\n",
      "1500 user processed.\n",
      "2000 user processed.\n",
      "2500 user processed.\n",
      "Finish.\n",
      "Totally 2963 users\n",
      "Totally 40730 reviews. Among them 1 reviews has empty true label sentence\n",
      "During constructing, 35679 user-item pair are being cutted due to their length\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "sample_sent_num = 500\n",
    "user_item_candidate_sent_ids_validset = dict()\n",
    "cnt_empty_true_sentence = 0\n",
    "user_cnt = 0\n",
    "review_cnt = 0\n",
    "user_item_candidate_sentence_num = list()\n",
    "cnt_being_cut_useritem = 0\n",
    "# Loop over all users\n",
    "user_cnt = 0\n",
    "for user_df_chunk in list(group_by_user_test):\n",
    "    user_id = int(user_df_chunk[0])\n",
    "    user_id_str = str(user_df_chunk[0])\n",
    "    user_df = user_df_chunk[1]\n",
    "    # get user sentences, these sentences are on TRAIN set\n",
    "    cur_user_sent_ids = set(trainset_user_to_sent_id[user_id_str])\n",
    "    # item-level dict\n",
    "    item_candidate_sent_ids = dict()\n",
    "    for idx, row in user_df.iterrows():\n",
    "        item_id = int(row['item'])\n",
    "        item_id_str = str(row['item'])\n",
    "        review_text = row['review']\n",
    "        review_cnt += 1\n",
    "        # get item sentences, they are on TRAIN set\n",
    "        cur_item_sent_ids = set(trainset_item_to_sent_id[item_id_str])\n",
    "        # get review_text's sent ids, they are on TEST set\n",
    "        cur_review_sent_ids = set()\n",
    "        ## tokenize this review\n",
    "        review_sents = sent_tokenize(review_text)\n",
    "        ## check whether this sentence is in the testset_sent_to_id dict\n",
    "        for sent in review_sents:\n",
    "            if sent in testset_sent_to_id:\n",
    "                cur_sent_id = testset_sent_to_id[sent]\n",
    "                # add this sentence into the set of current review\n",
    "                cur_review_sent_ids.add(cur_sent_id)\n",
    "        # construct the candidate set which is an union of user sentence and item sentence\n",
    "        cur_useritem_sent_ids = cur_user_sent_ids | cur_item_sent_ids\n",
    "        # sample some sentences (they are on TRAIN set)\n",
    "        if len(cur_useritem_sent_ids) > sample_sent_num:\n",
    "            sample_useritem_sent_ids = set(random.sample(cur_useritem_sent_ids, sample_sent_num))\n",
    "            cnt_being_cut_useritem += 1\n",
    "        else:\n",
    "            # FIXED!!\n",
    "            sample_useritem_sent_ids = cur_useritem_sent_ids\n",
    "        # add this into the dict\n",
    "        if len(cur_review_sent_ids) != 0:\n",
    "            item_candidate_sent_ids[item_id_str] = [list(sample_useritem_sent_ids), list(cur_review_sent_ids)]\n",
    "            user_item_candidate_sentence_num.append(len(cur_useritem_sent_ids))\n",
    "        else:\n",
    "            cnt_empty_true_sentence += 1\n",
    "\n",
    "    # add this item-level dict into the user-level dict\n",
    "    user_item_candidate_sent_ids_validset[user_id_str] = item_candidate_sent_ids\n",
    "    user_cnt += 1\n",
    "    if user_cnt % 500 == 0:\n",
    "        print(\"{} user processed.\".format(user_cnt))\n",
    "\n",
    "print('Finish.')\n",
    "print('Totally {} users'.format(user_cnt))\n",
    "print('Totally {0} reviews. Among them {1} reviews has empty true label sentence'.format(\n",
    "    review_cnt, cnt_empty_true_sentence))\n",
    "print('During constructing, {} user-item pair are being cutted due to their length'.format(cnt_being_cut_useritem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally 40729 user item pairs in the testset\n",
      "max number of candidate sentence: 7010\n",
      "min number of candidate sentence: 83\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(\"Totally {} user item pairs in the testset\".format(len(user_item_candidate_sentence_num)))\n",
    "print(\"max number of candidate sentence: {}\".format(np.max(user_item_candidate_sentence_num)))\n",
    "print(\"min number of candidate sentence: {}\".format(np.min(user_item_candidate_sentence_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "[5613,\n 5633,\n 5643,\n 5651,\n 5694,\n 5721,\n 5742,\n 5756,\n 5767,\n 5804,\n 5894,\n 5898,\n 5903,\n 5928,\n 5983,\n 6117,\n 6118,\n 6348,\n 6351,\n 7010]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(user_item_candidate_sentence_num)[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2963"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_item_candidate_sent_ids_validset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this into json file\n",
    "with open('../Dataset/ratebeer/{}/valid/useritem2sentids_test.json'.format(dataset_name), 'w') as f:\n",
    "    json.dump(user_item_candidate_sent_ids_validset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 1988 \t item: 202\n",
      "number of sentence in candidate set: 500\n",
      "number of sentence in true review set: 4\n"
     ]
    }
   ],
   "source": [
    "check_user_id = \"1988\"\n",
    "check_item_id = \"202\"\n",
    "print(\"user: {0} \\t item: {1}\".format(check_user_id, check_item_id))\n",
    "print(\"number of sentence in candidate set: {}\".format(len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][0])))\n",
    "print(\"number of sentence in true review set: {}\".format(len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reviews: 40729\n",
      "Total number of user: 2963\n",
      "Total number of item: 3669\n"
     ]
    }
   ],
   "source": [
    "# Checking How Many User/Item/Review are in the test set\n",
    "cnt_user = 0\n",
    "cnt_review = 0\n",
    "cnt_item_set = set()\n",
    "for trainset_user_chunk in list(user_item_candidate_sent_ids_validset.items()):\n",
    "    user_id_str = str(trainset_user_chunk[0])\n",
    "    user_id = int(trainset_user_chunk[0])\n",
    "    user_item_chunks = list(trainset_user_chunk[1].items())\n",
    "    for item_chunk in user_item_chunks:\n",
    "        item_id_str = str(item_chunk[0])\n",
    "        item_id = int(item_chunk[0])\n",
    "        # candidate_true_sent_ids = item_chunk[1]\n",
    "        # cur_data_dict = {'user_id': user_id, 'item_id': item_id, 'sent_id': candidate_true_sent_ids}\n",
    "        # write this into the json file\n",
    "        # json.dump(cur_data_dict, f1)\n",
    "        # f1.write(\"\\n\")\n",
    "        # assert user_id_str in train_user_id_set\n",
    "        # assert item_id_str in train_item_id_set\n",
    "        cnt_item_set.add(item_id_str)\n",
    "        cnt_review += 1\n",
    "    cnt_user += 1\n",
    "\n",
    "print(\"Total number of reviews: {}\".format(cnt_review))\n",
    "print(\"Total number of user: {}\".format(cnt_user))\n",
    "print(\"Total number of item: {}\".format(len(cnt_item_set)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 2963 users\n",
      "Totat 40729 reviews\n"
     ]
    }
   ],
   "source": [
    "# Write useritem2sentids_test into a line-by-line format\n",
    "with open('../Dataset/ratebeer/{}/valid/useritem2sentids_test_multilines.json'.format(dataset_name), 'w') as f1:\n",
    "    cnt_user = 0\n",
    "    cnt_review = 0\n",
    "    user_set = set()\n",
    "    useritem_set = set()\n",
    "    for trainset_user_chunk in list(user_item_candidate_sent_ids_validset.items()):\n",
    "        user_id_str = str(trainset_user_chunk[0])\n",
    "        user_id = int(trainset_user_chunk[0])\n",
    "        user_item_chunks = list(trainset_user_chunk[1].items())\n",
    "        for item_chunk in user_item_chunks:\n",
    "            item_id_str = str(item_chunk[0])\n",
    "            item_id = int(item_chunk[0])\n",
    "            candidate_sent_ids = item_chunk[1][0]\n",
    "            true_revw_sent_ids = item_chunk[1][1]\n",
    "            cur_data_dict = {'user_id':user_id, 'item_id':item_id, 'candidate':candidate_sent_ids, \"review\":true_revw_sent_ids}\n",
    "            # write this into the json file\n",
    "            json.dump(cur_data_dict, f1)\n",
    "            f1.write(\"\\n\")\n",
    "            cnt_review += 1\n",
    "            useritem_set.add((user_id_str, item_id_str))\n",
    "        cnt_user += 1\n",
    "        user_set.add(user_id_str)\n",
    "\n",
    "print(\"Total {} users\".format(cnt_user))\n",
    "print(\"Totat {} reviews\".format(cnt_review))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500 user processed.\n",
      "1000 user processed.\n",
      "1500 user processed.\n",
      "2000 user processed.\n",
      "2500 user processed.\n",
      "Finish.\n",
      "Totally 2963 users\n",
      "Totally 40730 reviews. Among them 1 reviews has empty true label sentence\n",
      "During constructing, 4079 user-item pair are being cutted due to their length\n"
     ]
    }
   ],
   "source": [
    "sample_sent_num = 2600\n",
    "user_item_candidate_sent_ids_testset = dict()\n",
    "cnt_empty_true_sentence = 0\n",
    "user_cnt = 0\n",
    "review_cnt = 0\n",
    "user_item_candidate_sentence_num = list()\n",
    "cnt_being_cut_useritem = 0\n",
    "# Loop over all users\n",
    "user_cnt = 0\n",
    "for user_df_chunk in list(group_by_user_test):\n",
    "    user_id = int(user_df_chunk[0])\n",
    "    user_id_str = str(user_df_chunk[0])\n",
    "    user_df = user_df_chunk[1]\n",
    "    # get user sentences, these sentences are on TRAIN set\n",
    "    cur_user_sent_ids = set(trainset_user_to_sent_id[user_id_str])\n",
    "    # item-level dict\n",
    "    item_candidate_sent_ids = dict()\n",
    "    for idx, row in user_df.iterrows():\n",
    "        item_id = int(row['item'])\n",
    "        item_id_str = str(row['item'])\n",
    "        review_text = row['review']\n",
    "        review_cnt += 1\n",
    "        # get item sentences, they are on TRAIN set\n",
    "        cur_item_sent_ids = set(trainset_item_to_sent_id[item_id_str])\n",
    "        # get review_text's sent ids, they are on TEST set\n",
    "        cur_review_sent_ids = set()\n",
    "        ## tokenize this review\n",
    "        review_sents = sent_tokenize(review_text)\n",
    "        ## check whether this sentence is in the testset_sent_to_id dict\n",
    "        for sent in review_sents:\n",
    "            if sent in testset_sent_to_id:\n",
    "                cur_sent_id = testset_sent_to_id[sent]\n",
    "                # add this sentence into the set of current review\n",
    "                cur_review_sent_ids.add(cur_sent_id)\n",
    "        # set union\n",
    "        cur_useritem_sent_ids = cur_user_sent_ids | cur_item_sent_ids\n",
    "        # sample some sentences (they are on TRAIN set)\n",
    "        if len(cur_useritem_sent_ids) > sample_sent_num:\n",
    "            sample_useritem_sent_ids = set(random.sample(cur_useritem_sent_ids, sample_sent_num))\n",
    "            cnt_being_cut_useritem += 1\n",
    "        else:\n",
    "            # FIXED!!\n",
    "            # sample_useritem_sent_ids = cur_user_sent_ids\n",
    "            sample_useritem_sent_ids = cur_useritem_sent_ids\n",
    "        # add this into the dict\n",
    "        if len(cur_review_sent_ids) != 0:\n",
    "            item_candidate_sent_ids[item_id_str] = [list(sample_useritem_sent_ids), list(cur_review_sent_ids)]\n",
    "            user_item_candidate_sentence_num.append(len(cur_useritem_sent_ids))\n",
    "        else:\n",
    "            cnt_empty_true_sentence += 1\n",
    "\n",
    "    # add this item-level dict into the user-level dict\n",
    "    user_item_candidate_sent_ids_testset[user_id_str] = item_candidate_sent_ids\n",
    "    user_cnt += 1\n",
    "    if user_cnt % 500 == 0:\n",
    "        print(\"{} user processed.\".format(user_cnt))\n",
    "\n",
    "print('Finish.')\n",
    "print('Totally {} users'.format(user_cnt))\n",
    "print('Totally {0} reviews. Among them {1} reviews has empty true label sentence'.format(\n",
    "    review_cnt, cnt_empty_true_sentence))\n",
    "print('During constructing, {} user-item pair are being cutted due to their length'.format(cnt_being_cut_useritem))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally 40729 user item pairs in the testset\n",
      "max number of candidate sentence: 7010\n",
      "min number of candidate sentence: 83\n"
     ]
    }
   ],
   "source": [
    "print(\"Totally {} user item pairs in the testset\".format(len(user_item_candidate_sentence_num)))\n",
    "print(\"max number of candidate sentence: {}\".format(np.max(user_item_candidate_sentence_num)))\n",
    "print(\"min number of candidate sentence: {}\".format(np.min(user_item_candidate_sentence_num)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2611"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(user_item_candidate_sentence_num)[-4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2963"
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(user_item_candidate_sent_ids_testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this into json file\n",
    "with open('../Dataset/ratebeer/{}/test/useritem2sentids_test.json'.format(dataset_name), 'w') as f:\n",
    "    json.dump(user_item_candidate_sent_ids_testset, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40729\n"
     ]
    }
   ],
   "source": [
    "review_test_cnt = 0\n",
    "for user_chunk in user_item_candidate_sent_ids_testset.items():\n",
    "    user_id = user_chunk[0]\n",
    "    user_dict = user_chunk[1]\n",
    "    for user_item_chunk in user_dict.items():\n",
    "        item_id = user_item_chunk[0]\n",
    "        candidate_sents = user_item_chunk[0]\n",
    "        true_label_sents = user_item_chunk[1]\n",
    "        review_test_cnt += 1\n",
    "print(review_test_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 1988 \t item: 202\n",
      "number of sentence in candidate set: 2248\n",
      "number of sentence in true review set: 4\n"
     ]
    }
   ],
   "source": [
    "check_user_id = \"1988\"\n",
    "check_item_id = \"202\"\n",
    "print(\"user: {0} \\t item: {1}\".format(check_user_id, check_item_id))\n",
    "print(\"number of sentence in candidate set: {}\".format(len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][0])))\n",
    "print(\"number of sentence in true review set: {}\".format(len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total 2963 users\n",
      "Totat 40729 reviews\n"
     ]
    }
   ],
   "source": [
    "# Write useritem2sentids_test into a line-by-line format\n",
    "with open('../Dataset/ratebeer/{}/test/useritem2sentids_test_multilines.json'.format(dataset_name), 'w') as f1:\n",
    "    cnt_user = 0\n",
    "    cnt_review = 0\n",
    "    user_set = set()\n",
    "    useritem_set = set()\n",
    "    for trainset_user_chunk in list(user_item_candidate_sent_ids_testset.items()):\n",
    "        user_id_str = str(trainset_user_chunk[0])\n",
    "        user_id = int(trainset_user_chunk[0])\n",
    "        user_item_chunks = list(trainset_user_chunk[1].items())\n",
    "        for item_chunk in user_item_chunks:\n",
    "            item_id_str = str(item_chunk[0])\n",
    "            item_id = int(item_chunk[0])\n",
    "            candidate_sent_ids = item_chunk[1][0]\n",
    "            true_revw_sent_ids = item_chunk[1][1]\n",
    "            cur_data_dict = {'user_id':user_id, 'item_id':item_id, 'candidate':candidate_sent_ids, \"review\":true_revw_sent_ids}\n",
    "            # write this into the json file\n",
    "            json.dump(cur_data_dict, f1)\n",
    "            f1.write(\"\\n\")\n",
    "            cnt_review += 1\n",
    "            useritem_set.add((user_id_str, item_id_str))\n",
    "        cnt_user += 1\n",
    "        user_set.add(user_id_str)\n",
    "\n",
    "print(\"Total {} users\".format(cnt_user))\n",
    "print(\"Totat {} reviews\".format(cnt_review))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: 1988 \t item: 202\n",
      "[VALID] number of sentence in candidate set: 500\n",
      "[VALID] number of sentence in true review set: 4\n",
      "[TEST]  number of sentence in candidate set: 2248\n",
      "[TEST]  number of sentence in true review set: 4\n"
     ]
    }
   ],
   "source": [
    "check_user_id = \"1988\"\n",
    "check_item_id = \"202\"\n",
    "print(\"user: {0} \\t item: {1}\".format(check_user_id, check_item_id))\n",
    "print(\"[VALID] number of sentence in candidate set: {}\".format(len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][0])))\n",
    "print(\"[VALID] number of sentence in true review set: {}\".format(len(user_item_candidate_sent_ids_validset[check_user_id][check_item_id][1])))\n",
    "print(\"[TEST]  number of sentence in candidate set: {}\".format(len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][0])))\n",
    "print(\"[TEST]  number of sentence in true review set: {}\".format(len(user_item_candidate_sent_ids_testset[check_user_id][check_item_id][1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "263"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset_user_to_sent_id[check_user_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1985"
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset_item_to_sent_id[check_item_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "2248"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(trainset_user_to_sent_id[check_user_id]) | set(trainset_item_to_sent_id[check_item_id]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('2019.10': virtualenv)",
   "name": "python374jvsc74a57bd020bbdedb0079e23a85211f14a09dbcc829fefc965ff02508ebf68fe08b48d387"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "20bbdedb0079e23a85211f14a09dbcc829fefc965ff02508ebf68fe08b48d387"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}